{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Scientific Deep Dive Into SageMaker LDA\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Setup](#Setup)\n",
    "1. [Training](#Training)\n",
    "1. [Inference](#Inference)\n",
    "1. [Epilogue](#Epilogue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "***\n",
    "\n",
    "Amazon SageMaker LDA is an unsupervised learning algorithm that attempts to describe a set of observations as a mixture of distinct categories. Latent Dirichlet Allocation (LDA) is most commonly used to discover a user-specified number of topics shared by documents within a text corpus. Here each observation is a document, the features are the presence (or occurrence count) of each word, and the categories are the topics. Since the method is unsupervised, the topics are not specified up front, and are not guaranteed to align with how a human may naturally categorize documents. The topics are learned as a probability distribution over the words that occur in each document. Each document, in turn, is described as a mixture of topics.\n",
    "\n",
    "This notebook is similar to **LDA-Introduction.ipynb** but its objective and scope are a different. We will be taking a deeper dive into the theory. The primary goals of this notebook are,\n",
    "\n",
    "* to understand the LDA model and the example dataset,\n",
    "* understand how the Amazon SageMaker LDA algorithm works,\n",
    "* interpret the meaning of the inference output.\n",
    "\n",
    "Former knowledge of LDA is not required. However, we will run through concepts rather quickly and at least a foundational knowledge of mathematics or machine learning is recommended. Suggested references are provided, as appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -y scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jieba\n",
    "!pip install ckiptagger \n",
    "!pip install tensorflow==1.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os, re, tarfile\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "# accessing the SageMaker Python SDK\n",
    "from sagemaker.amazon.common import numpy_to_record_serializer\n",
    "from sagemaker.predictor import csv_serializer, json_deserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 100 /home/ec2-user/SageMaker/chinese-corpus/toutiao_cat_data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat /home/ec2-user/SageMaker/chinese-corpus/toutiao_cat_data.txt | grep '_news_entertainment_' > toutiao_cat_data_ent.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jieba\n",
    "!pip install ckiptagger \n",
    "!pip install tensorflow==1.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba.posseg as pseg\n",
    "import json \n",
    "import numpy \n",
    "import sys \n",
    "max_feature_dim = 5000 \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "sys.path.append('/home/ec2-user/SageMaker/nlp_processing/')\n",
    "\n",
    "from preprocess import TouTiaoNewsPreprocessor\n",
    "from segmenter import JiebaSegmenter\n",
    "from transformer import BlazingTextInputDataTransformer\n",
    "\n",
    "\n",
    "\n",
    "def to_dense_vectors(document_str, vocab_list): \n",
    "    vocab_dict = {} \n",
    "    vocab_inverse_dict = {} \n",
    "    for i, v in enumerate(vocab_list):\n",
    "        vocab_dict[v] = i \n",
    "        vocab_inverse_dict[i] = v\n",
    "        \n",
    "    feature_dim = len(vocab_dict)\n",
    "    vectors = [] \n",
    "    for d in document_str: \n",
    "        toks = d.split(' ')\n",
    "        \n",
    "        res = numpy.zeros(feature_dim)\n",
    "        for t in toks:\n",
    "            if t in vocab_dict:\n",
    "                idx = vocab_dict[t]\n",
    "                res[idx] += 1 \n",
    "        vectors.append(res)\n",
    "    return numpy.asarray(vectors),vocab_inverse_dict \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "def feature_selection(documents, max_feature_dim): \n",
    "    vectorizer = CountVectorizer(input='content', analyzer='word', stop_words='english',max_features=max_feature_dim, max_df=0.95, min_df=2)\n",
    "    vectors = vectorizer.fit_transform(documents)\n",
    "    vocab_list = vectorizer.get_feature_names()\n",
    "\n",
    "    vectors = sparse.csr_matrix(vectors, dtype=np.float32)\n",
    "    return vectors, vocab_list\n",
    "\n",
    "            \n",
    "\n",
    "preprocessor = TouTiaoNewsPreprocessor()\n",
    "res = preprocessor.preprocess('toutiao_cat_data_ent.txt')\n",
    "segmenter = JiebaSegmenter()\n",
    "document_str = [] \n",
    "for r in res:\n",
    "    toks = segmenter.segment(r[2])\n",
    "    document_str.append(' '.join(toks))    \n",
    "    \n",
    "print (document_str[0:10])    \n",
    "documents, vocab_list = feature_selection(document_str, max_feature_dim)\n",
    "vocabulary_size = len(vocab_list)\n",
    "dense_docs,inverse_word_index = to_dense_vectors(document_str, vocab_list)\n",
    "print(dense_docs[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dense_docs))\n",
    "print(type(dense_docs))\n",
    "print(type(dense_docs[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('First training document =\\n{}'.format(dense_docs[0]))\n",
    "print('\\nVocabulary size = {}'.format(vocabulary_size))\n",
    "print('Length of first document = {}'.format(dense_docs[0].sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The LDA Model\n",
    "\n",
    "As mentioned above, LDA is a model for discovering latent topics describing a collection of documents. In this section we will give a brief introduction to the model. Let,\n",
    "\n",
    "* $M$ = the number of *documents* in a corpus\n",
    "* $N$ = the average *length* of a document.\n",
    "* $V$ = the size of the *vocabulary* (the total number of unique words)\n",
    "\n",
    "We denote a *document* by a vector $w \\in \\mathbb{R}^V$ where $w_i$ equals the number of times the $i$th word in the vocabulary occurs within the document. This is called the \"bag-of-words\" format of representing a document.\n",
    "\n",
    "$$\n",
    "\\underbrace{w}_{\\text{document}} = \\overbrace{\\big[ w_1, w_2, \\ldots, w_V \\big] }^{\\text{word counts}},\n",
    "\\quad\n",
    "V = \\text{vocabulary size}\n",
    "$$\n",
    "\n",
    "The *length* of a document is equal to the total number of words in the document: $N_w = \\sum_{i=1}^V w_i$.\n",
    "\n",
    "An LDA model is defined by two parameters: a topic-word distribution matrix $\\beta \\in \\mathbb{R}^{K \\times V}$ and a  Dirichlet topic prior $\\alpha \\in \\mathbb{R}^K$. In particular, let,\n",
    "\n",
    "$$\\beta = \\left[ \\beta_1, \\ldots, \\beta_K \\right]$$\n",
    "\n",
    "be a collection of $K$ *topics* where each topic $\\beta_k \\in \\mathbb{R}^V$ is represented as probability distribution over the vocabulary. One of the utilities of the LDA model is that a given word is allowed to appear in multiple topics with positive probability. The Dirichlet topic prior is a vector $\\alpha \\in \\mathbb{R}^K$ such that $\\alpha_k > 0$ for all $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Documents\n",
    "\n",
    "LDA is a generative model, meaning that the LDA parameters $(\\alpha, \\beta)$ are used to construct documents word-by-word by drawing from the topic-word distributions. In fact, looking closely at the example documents above you can see that some documents sample more words from some topics than from others.\n",
    "\n",
    "LDA works as follows: given \n",
    "\n",
    "* $M$ documents $w^{(1)}, w^{(2)}, \\ldots, w^{(M)}$,\n",
    "* an average document length of $N$,\n",
    "* and an LDA model $(\\alpha, \\beta)$.\n",
    "\n",
    "**For** each document, $w^{(m)}$:\n",
    "* sample a topic mixture: $\\theta^{(m)} \\sim \\text{Dirichlet}(\\alpha)$\n",
    "* **For** each word $n$ in the document:\n",
    "  * Sample a topic $z_n^{(m)} \\sim \\text{Multinomial}\\big( \\theta^{(m)} \\big)$\n",
    "  * Sample a word from this topic, $w_n^{(m)} \\sim \\text{Multinomial}\\big( \\beta_{z_n^{(m)}} \\; \\big)$\n",
    "  * Add to document\n",
    "\n",
    "The [plate notation](https://en.wikipedia.org/wiki/Plate_notation) for the LDA model, introduced in [2], encapsulates this process pictorially.\n",
    "\n",
    "![](http://scikit-learn.org/stable/_images/lda_model_graph.png)\n",
    "\n",
    "> [2] David M Blei, Andrew Y Ng, and Michael I Jordan. Latent Dirichlet Allocation. Journal of Machine Learning Research, 3(Jan):993â€“1022, 2003."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Mixtures\n",
    "\n",
    "For the documents we generated above lets look at their corresponding topic mixtures, $\\theta \\in \\mathbb{R}^K$. The topic mixtures represent the probablility that a given word of the document is sampled from a particular topic. For example, if the topic mixture of an input document $w$ is,\n",
    "\n",
    "$$\\theta = \\left[ 0.3, 0.2, 0, 0.5, 0, \\ldots, 0 \\right]$$\n",
    "\n",
    "then $w$ is 30% generated from the first topic, 20% from the second topic, and 50% from the fourth topic. In particular, the words contained in the document are sampled from the first topic-word probability distribution 30% of the time, from the second distribution 20% of the time, and the fourth disribution 50% of the time.\n",
    "\n",
    "\n",
    "The objective of inference, also known as scoring, is to determine the most likely topic mixture of a given input document. Colloquially, this means figuring out which topics appear within a given document and at what ratios. We will perform infernece later in the [Inference](#Inference) section.\n",
    "\n",
    "Since we generated these example documents using the LDA model we know the topic mixture generating them. Let's examine these topic mixtures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "***\n",
    "\n",
    "In this section we will give some insight into how AWS SageMaker LDA fits an LDA model to a corpus, create an run a SageMaker LDA training job, and examine the output trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "***\n",
    "\n",
    "*This notebook was created and tested on an ml.m4.xlarge notebook instance.*\n",
    "\n",
    "We first need to specify some AWS credentials; specifically data locations and access roles. This is the only cell of this notebook that you will need to edit. In particular, we need the following data:\n",
    "\n",
    "* `bucket` - An S3 bucket accessible by this account.\n",
    "  * Used to store input training data and model data output.\n",
    "  * Should be withing the same region as this notebook instance, training, and hosting.\n",
    "* `prefix` - The location in the bucket where this notebook's input and and output data will be stored. (The default value is sufficient.)\n",
    "* `role` - The IAM Role ARN used to give training and hosting access to your data.\n",
    "  * See documentation on how to create these.\n",
    "  * The script below will try to determine an appropriate Role ARN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true,
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "\n",
    "# bucket = 'cht-ws-nlp-yianc'\n",
    "print(bucket)\n",
    "role = get_execution_role()\n",
    "print(role) # This is the role that SageMaker would use to leverage AWS resources (S3, CloudWatch) on your behalf\n",
    "\n",
    "prefix = 'lda/entertainment_news' #Replace with the prefix under which you want to store the data if needed\n",
    "\n",
    "print('Training input/output will be stored in {}/{}'.format(bucket, prefix))\n",
    "print('\\nIAM Role: {}'.format(role))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Data on S3\n",
    "\n",
    "Before we run training we need to prepare the data.\n",
    "\n",
    "A SageMaker training job needs access to training data stored in an S3 bucket. Although training can accept data of various formats we convert the documents MXNet RecordIO Protobuf format before uploading to the S3 bucket defined at the beginning of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_to_protobuf_and_upload(sparray, bucket, prefix):\n",
    "    recordio_protobuf_serializer = numpy_to_record_serializer()\n",
    "    fbuffer = recordio_protobuf_serializer(sparray)\n",
    "\n",
    "    # upload to S3 in bucket/prefix/train\n",
    "    fname = 'lda.data'\n",
    "    s3_object = os.path.join(prefix, 'train', fname)\n",
    "    boto3.Session().resource('s3').Bucket(bucket).Object(s3_object).upload_fileobj(fbuffer)\n",
    "\n",
    "    s3_train_data = 's3://{}/{}'.format(bucket, s3_object)\n",
    "    print('Uploaded data to S3: {}'.format(s3_train_data))\n",
    "    return s3_train_data\n",
    "\n",
    "    \n",
    "s3_train_data = numpy_to_protobuf_and_upload(dense_docs, bucket, prefix)    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we specify a Docker container containing the SageMaker LDA algorithm. For your convenience, a region-specific container is automatically chosen for you to minimize cross-region data communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "region_name = boto3.Session().region_name\n",
    "container = get_image_uri(boto3.Session().region_name, 'lda')\n",
    "\n",
    "print('Using SageMaker LDA container: {} ({})'.format(container, region_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Parameters\n",
    "\n",
    "Particular to a SageMaker LDA training job are the following hyperparameters:\n",
    "\n",
    "* **`num_topics`** - The number of topics or categories in the LDA model.\n",
    "  * Usually, this is not known a priori.\n",
    "  * In this example, howevever, we know that the data is generated by five topics.\n",
    "\n",
    "* **`feature_dim`** - The size of the *\"vocabulary\"*, in LDA parlance.\n",
    "  * In this example, this is equal 25.\n",
    "\n",
    "* **`mini_batch_size`** - The number of input training documents.\n",
    "\n",
    "* **`alpha0`** - *(optional)* a measurement of how \"mixed\" are the topic-mixtures.\n",
    "  * When `alpha0` is small the data tends to be represented by one or few topics.\n",
    "  * When `alpha0` is large the data tends to be an even combination of several or many topics.\n",
    "  * The default value is `alpha0 = 1.0`.\n",
    "\n",
    "In addition to these LDA model hyperparameters, we provide additional parameters defining things like the EC2 instance type on which training will run, the S3 bucket containing the data, and the AWS access role. Note that,\n",
    "\n",
    "* Recommended instance type: `ml.c4`\n",
    "* Current limitations:\n",
    "  * SageMaker LDA *training* can only run on a single instance.\n",
    "  * SageMaker LDA does not take advantage of GPU hardware.\n",
    "  * (The Amazon AI Algorithms team is working hard to provide these capabilities in a future release!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above configuration create a SageMaker client and use the client to create a training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = sagemaker.Session()\n",
    "\n",
    "document_nums = len(dense_docs)\n",
    "# specify general training job information\n",
    "lda = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.m5.xlarge',\n",
    "    sagemaker_session=session,\n",
    ")\n",
    "num_topics = 20\n",
    "# set algorithm-specific hyperparameters\n",
    "lda.set_hyperparameters(\n",
    "    num_topics=num_topics,\n",
    "    feature_dim=vocabulary_size,\n",
    "    mini_batch_size=document_nums,\n",
    "    alpha0=1.0,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "from sagemaker.session import s3_input\n",
    "s3_train = s3_input(s3_train_data) \n",
    "\n",
    "# run the training job on input data stored in S3\n",
    "lda.fit({'train': s3_train})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see the message\n",
    "\n",
    "> `===== Job Complete =====`\n",
    "\n",
    "at the bottom of the output logs then that means training sucessfully completed and the output LDA model was stored in the specified output path. You can also view information about and the status of a training job using the AWS SageMaker console. Just click on the \"Jobs\" tab and select training job matching the training job name, below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Training job name: {}'.format(lda.latest_training_job.job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "***\n",
    "\n",
    "A trained model does nothing on its own. We now want to use the model we computed to perform inference on data. For this example, that means predicting the topic mixture representing a given document.\n",
    "\n",
    "We create an inference endpoint using the SageMaker Python SDK `deploy()` function from the job we defined above. We specify the instance type where inference is computed as well as an initial number of instances to spin up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_inference = lda.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',  # LDA inference may work better at scale on ml.c4 instances\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You now have a functioning SageMaker LDA inference endpoint. You can confirm the endpoint configuration and status by navigating to the \"Endpoints\" tab in the AWS SageMaker console and selecting the endpoint matching the endpoint name, below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Endpoint name: {}'.format(lda_inference.endpoint))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this realtime endpoint at our fingertips we can finally perform inference on our training and test data.\n",
    "\n",
    "We can pass a variety of data formats to our inference endpoint. In this example we will demonstrate passing CSV-formatted data. Other available formats are JSON-formatted, JSON-sparse-formatter, and RecordIO Protobuf. We make use of the SageMaker Python SDK utilities `csv_serializer` and `json_deserializer` when configuring the inference endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_inference.content_type = 'text/csv'\n",
    "lda_inference.serializer = csv_serializer\n",
    "lda_inference.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass some test documents to the inference endpoint. Note that the serializer and deserializer will atuomatically take care of the datatype conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = lda_inference.predict(dense_docs[:12])\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may be hard to see but the output format of SageMaker LDA inference endpoint is a Python dictionary with the following format.\n",
    "\n",
    "```\n",
    "{\n",
    "  'predictions': [\n",
    "    {'topic_mixture': [ ... ] },\n",
    "    {'topic_mixture': [ ... ] },\n",
    "    {'topic_mixture': [ ... ] },\n",
    "    ...\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "We extract the topic mixtures, themselves, corresponding to each of the input documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Analysis\n",
    "\n",
    "Recall that although SageMaker LDA successfully learned the underlying topics which generated the sample data the topics were in a different order. Before we compare to known topic mixtures $\\theta \\in \\mathbb{R}^K$ we should also permute the inferred topic mixtures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot these topic mixture probability distributions alongside the known ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array([prediction['topic_mixture'] for prediction in results['predictions']])\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fs = 12\n",
    "df=pd.DataFrame(predictions.T)\n",
    "df.plot(kind='bar', figsize=(16,4), fontsize=fs)\n",
    "plt.ylabel('Topic assignment', fontsize=fs+2)\n",
    "plt.xlabel('Topic ID', fontsize=fs+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the Trained Model\n",
    "\n",
    "We know the LDA parameters $(\\alpha, \\beta)$ used to generate the example data. How does the learned model compare the known one? In this section we will download the model data and measure how well SageMaker LDA did in learning the model.\n",
    "\n",
    "First, we download the model data. SageMaker will output the model in \n",
    "\n",
    "> `s3://<bucket>/<prefix>/output/<training job name>/output/model.tar.gz`.\n",
    "\n",
    "SageMaker LDA stores the model as a two-tuple $(\\alpha, \\beta)$ where each LDA parameter is an MXNet NDArray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm model.tar.gz\n",
    "!rm model_algo*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # download and extract the model file from S3\n",
    "import mxnet as mx\n",
    "job_name = lda.latest_training_job.job_name\n",
    "print(job_name)\n",
    "model_fname = 'model.tar.gz'\n",
    "model_object = os.path.join(prefix, 'output', job_name, 'output', model_fname)\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(model_object).download_file(model_fname)\n",
    "model_object = os.path.join(prefix, 'output', job_name, 'output', model_fname)\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(model_object).download_file(model_fname)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_object = os.path.join(prefix, 'output', job_name, 'output', model_fname)\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(model_object).download_file(model_fname)\n",
    "with tarfile.open(model_fname) as tar:\n",
    "    tar.extractall()\n",
    "print('Downloaded and extracted model tarball: {}'.format(model_object))\n",
    "\n",
    "# obtain the model file\n",
    "model_list = [fname for fname in os.listdir('.') if fname.startswith('model_')]\n",
    "model_fname = model_list[0]\n",
    "print('Found model file: {}'.format(model_fname))\n",
    "\n",
    "# get the model from the model file and store in Numpy arrays\n",
    "alpha, beta = mx.ndarray.load('model_algo-1')\n",
    "learned_alpha_permuted = alpha.asnumpy()\n",
    "learned_beta_permuted = beta.asnumpy()\n",
    "\n",
    "print('\\nLearned alpha.shape = {}'.format(learned_alpha_permuted.shape))\n",
    "print('Learned beta.shape = {}'.format(learned_beta_permuted.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo yum install wqy-zenhei-fonts -y \n",
    "!sudo yum install cjkuni-fonts-common -y \n",
    "!sudo yum install cjkuni-fonts-ghostscript -y \n",
    "!sudo yum install cjkuni-ukai-fonts -y \n",
    "!sudo yum install cjkuni-uming-fonts -y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import PIL.Image as Image\n",
    "from wordcloud import WordCloud \n",
    "topics_to_show = []\n",
    "font_path = '/usr/share/fonts/cjkuni-ukai/ukai.ttc'\n",
    "def makeImage(text):\n",
    "    wc = WordCloud(font_path=font_path, background_color=\"white\", max_words=1000)\n",
    "    # generate word cloud\n",
    "    wc.generate_from_frequencies(text)\n",
    "\n",
    "    # show\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()      \n",
    "\n",
    "\n",
    "for beta in learned_beta_permuted: \n",
    "    topic_dic = {}\n",
    "    for i, wi in enumerate(beta): \n",
    "        wc = inverse_word_index[i]\n",
    "        topic_dic[wc] = wi*1000 \n",
    "    makeImage(topic_dic)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presumably, SageMaker LDA has found the topics most likely used to generate the training corpus. However, even if this is case the topics would not be returned in any particular order. Therefore, we match the found topics to the known topics closest in L1-norm in order to find the topic permutation.\n",
    "\n",
    "Note that we will use the `permutation` later during inference to match known topic mixtures to found topic mixtures.\n",
    "\n",
    "Below plot the known topic-word probability distribution, $\\beta \\in \\mathbb{R}^{K \\times V}$ next to the distributions found by SageMaker LDA as well as the L1-norm errors between the two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop / Close the Endpoint\n",
    "\n",
    "Finally, we should delete the endpoint before we close the notebook.\n",
    "\n",
    "To do so execute the cell below. Alternately, you can navigate to the \"Endpoints\" tab in the SageMaker console, select the endpoint with the name stored in the variable `endpoint_name`, and select \"Delete\" from the \"Actions\" dropdown menu. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(lda_inference.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epilogue\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook we,\n",
    "\n",
    "* learned about the LDA model,\n",
    "* generated some example LDA documents and their corresponding topic-mixtures,\n",
    "* trained a SageMaker LDA model on a training set of documents and compared the learned model to the known model,\n",
    "* created an inference endpoint,\n",
    "* used the endpoint to infer the topic mixtures of a test input and analyzed the inference error.\n",
    "\n",
    "There are several things to keep in mind when applying SageMaker LDA to real-word data such as a corpus of text documents. Note that input documents to the algorithm, both in training and inference, need to be vectors of integers representing word counts. Each index corresponds to a word in the corpus vocabulary. Therefore, one will need to \"tokenize\" their corpus vocabulary.\n",
    "\n",
    "$$\n",
    "\\text{\"cat\"} \\mapsto 0, \\; \\text{\"dog\"} \\mapsto 1 \\; \\text{\"bird\"} \\mapsto 2, \\ldots\n",
    "$$\n",
    "\n",
    "Each text document then needs to be converted to a \"bag-of-words\" format document.\n",
    "\n",
    "$$\n",
    "w = \\text{\"cat bird bird bird cat\"} \\quad \\longmapsto \\quad w = [2, 0, 3, 0, \\ldots, 0]\n",
    "$$\n",
    "\n",
    "Also note that many real-word applications have large vocabulary sizes. It may be necessary to represent the input documents in sparse format. Finally, the use of stemming and lemmatization in data preprocessing provides several benefits. Doing so can improve training and inference compute time since it reduces the effective vocabulary size. More importantly, though, it can improve the quality of learned topic-word probability matrices and inferred topic mixtures. For example, the words *\"parliament\"*, *\"parliaments\"*, *\"parliamentary\"*, *\"parliament's\"*, and *\"parliamentarians\"* are all essentially the same word, *\"parliament\"*, but with different conjugations. For the purposes of detecting topics, such as a *\"politics\"* or *governments\"* topic, the inclusion of all five does not add much additional value as they all essentiall describe the same feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
