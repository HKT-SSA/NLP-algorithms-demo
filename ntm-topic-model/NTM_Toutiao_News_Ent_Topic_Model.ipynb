{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to SageMaker Neural Topic Model\n",
    "\n",
    "***Unsupervised representation learning and topic extraction using Neural Topic Model***\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Data Preparation](#Data-Preparation)\n",
    "1. [Model Training](#Model-Training)\n",
    "1. [Model Hosting and Inference](#Model-Hosting-and-Inference)\n",
    "1. [Model Exploration](#Model-Exploration)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Introduction\n",
    "\n",
    "Amazon SageMaker Neural Topic Model (NTM) is an unsupervised learning algorithm that attempts to describe a set of observations as a mixture of distinct categories. NTM is most commonly used to discover a user-specified number of topics shared by documents within a text corpus. Here each observation is a document, the features are the presence (or occurrence count) of each word, and the categories are the topics. Since the method is unsupervised, the topics are not specified upfront and are not guaranteed to align with how a human may naturally categorize documents. The topics are learned as a probability distribution over the words that occur in each document. Each document, in turn, is described as a mixture of topics. \n",
    "\n",
    "In this notebook, we will use the Amazon SageMaker NTM algorithm to train a model on the [20NewsGroups](https://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups) data set. This data set has been widely used as a topic modeling benchmark. \n",
    "\n",
    "The main goals of this notebook are as follows:\n",
    "\n",
    "1. learn how to obtain and store data for use in Amazon SageMaker,\n",
    "2. create an AWS SageMaker training job on a data set to produce an NTM model,\n",
    "3. use the model to perform inference with an Amazon SageMaker endpoint.\n",
    "4. explore trained model and visualized learned topics\n",
    "\n",
    "If you would like to know more please check out the [SageMaker Neural Topic Model Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/ntm.html).\n",
    "\n",
    "### A Brief Overview of SageMaker NTM\n",
    "\n",
    "Topic models are a classical example of probablistic graphical models that involve challenging posterior inference problems. We implement topic modeling under a neural-network based variational inference framework. The difficult inference problem is framed as an optimization problem solved by scalable methods such as stochastic gradient descent. Compared to conventional inference schemes, the neural-network implementation allows for scalable model training as well as low-latency inference. Furthermore, the flexibility of the neural inference framework allows us to more quickly add new functionalities and serve a wider range of customer use cases.\n",
    "\n",
    "The high-level diagram of SageMaker NTM is shown below:\n",
    "\n",
    "<img src=\"ntm_diagram.png\" width=\"600\">\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\textrm{Encoder} \\ q(z\\vert x): & \\\\\n",
    "& \\pi = f_X^{MLP}(x), \\quad \\mu(x) = l_1(\\pi), \\quad \\log \\sigma(x) = l_2(\\pi)\\\\\n",
    "& h(x, \\epsilon) = \\mu + \\sigma \\epsilon, \\ \\textrm{where} \\ \\epsilon \\sim \\mathcal{N}(0,I),\\quad z=g(h), \\ \\textrm{where} \\ h \\sim \\mathcal{N}(\\mu, \\sigma^2 I) \\\\\n",
    "\\textrm{Decoder} \\ p(x\\vert z): & \\\\\n",
    "& y(z) = \\textrm{softmax}(Wz+b), \\quad \\log p(x\\vert z) = \\sum x \\odot \\log(y(z))\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where $l_1$ and $l_2$ are linear transformations with bias.\n",
    "\n",
    "### Beyond Text Data\n",
    "\n",
    "In principle, topic models can be applied to types of data beyond text documents. For example, topic modeling has been applied on network traffice data to discover [peer-to-peer applications usage patterns](http://isis.poly.edu/~baris/papers/GangsOfInternet-CNS13.pdf). We would be glad to hear about your novel use cases and are happy to help provide additional information. Please feel free to post questions or feedback at our [GitHub repository](https://github.com/awslabs/amazon-sagemaker-examples) or in the [Amazon SageMaker](https://forums.aws.amazon.com/forum.jspa?forumID=285) section of AWS Developer Forum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data Preparation TODO \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching Data Set\n",
    "\n",
    "First let's define the folder to hold the data and clean the content in it which might be from previous experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6551700932705387022_!_101_!_news_culture_!_京城最值得你来场文化之旅的博物馆_!_保利集团,马未都,中国科学技术馆,博物馆,新中国\r\n",
      "6552368441838272771_!_101_!_news_culture_!_发酵床的垫料种类有哪些？哪种更好？_!_\r\n",
      "6552407965343678723_!_101_!_news_culture_!_上联：黄山黄河黄皮肤黄土高原。怎么对下联？_!_\r\n",
      "6552332417753940238_!_101_!_news_culture_!_林徽因什么理由拒绝了徐志摩而选择梁思成为终身伴侣？_!_\r\n",
      "6552475601595269390_!_101_!_news_culture_!_黄杨木是什么树？_!_\r\n",
      "6552387648126714125_!_101_!_news_culture_!_上联：草根登上星光道，怎么对下联？_!_\r\n",
      "6552271725814350087_!_101_!_news_culture_!_什么是超写实绘画？_!_\r\n",
      "6552452982015787268_!_101_!_news_culture_!_松涛听雨莺婉转，下联？_!_\r\n",
      "6552400379030536455_!_101_!_news_culture_!_上联：老子骑牛读书，下联怎么对？_!_\r\n",
      "6552339283632455939_!_101_!_news_culture_!_上联：山水醉人何须酒。如何对下联？_!_\r\n",
      "6552387367334838792_!_101_!_news_culture_!_国画山水，如何读懂山水画_!_林风眠,黄海归来步步云,秋山图,计白当黑,山水画,江山万里图,张大千,巫峡清秋图,活眼,山雨欲来图\r\n",
      "6552314684383429128_!_101_!_news_culture_!_一元硬币是这种，现在价值24000元，找找看！_!_牡丹,收藏价值\r\n",
      "6552128476109865229_!_101_!_news_culture_!_有哪些让人感动的语句呢？_!_\r\n",
      "6552447749072093453_!_101_!_news_culture_!_上联，绿竹引清风，如何对下联？_!_\r\n",
      "6552301380562846215_!_101_!_news_culture_!_赵宁安作品欣赏_!_叶浅予,田世光,李苦禅,花鸟画,中央美术学院\r\n",
      "6552263394420850951_!_101_!_news_culture_!_夕阳无语燕归愁，如何接下句？_!_\r\n",
      "6552290314294395139_!_101_!_news_culture_!_上联：山水醉人何须酒。如何对下联？_!_\r\n",
      "6552462208314376462_!_101_!_news_culture_!_上联：上班为下班，如何对下联？_!_\r\n",
      "6552311866947797262_!_101_!_news_culture_!_下联:夕陽西下已黄昏。上联是什麽？_!_\r\n",
      "6552466638304707079_!_101_!_news_culture_!_初夏方好，小荷初露，一起来读那些美不胜收的咏荷经典诗词_!_荷花,西湖,金粟词话,采莲女,念奴娇·赤壁怀古,林逋,荷叶\r\n",
      "6552431613437805063_!_102_!_news_entertainment_!_谢娜为李浩菲澄清网络谣言，之后她的两个行为给自己加分_!_佟丽娅,网络谣言,快乐大本营,李浩菲,谢娜,观众们\r\n",
      "6552320560913711629_!_102_!_news_entertainment_!_谢娜曾为他与主办方撕破脸！谢娜复出快本！他第一个到场支持！_!_汪涵,火星情报局,杨迪,主办方,谢娜,刘维\r\n",
      "6552390546051039747_!_102_!_news_entertainment_!_中国网红竟红到美国？不多说了，连小编都心动了_!_飞纱,新娘,脱口秀,中国网,婚礼\r\n",
      "6552150358678831624_!_102_!_news_entertainment_!_赵丽颖很久没有登上微博热搜了，但你们别急，她只是在憋大招而已_!_陆贞传奇,大红大紫,楚乔传,微博热搜,赵丽颖,花千骨,迪丽热巴,Angelababy\r\n",
      "6552408585177924099_!_102_!_news_entertainment_!_因戴一个眼镜更改变气质的6大娱乐圈明星，你最喜欢哪一个_!_戴上眼镜,刘德华,张翰,远大前程,杜志国,刘亦菲\r\n",
      "6552147830184608263_!_102_!_news_entertainment_!_后来的我们，抢先看_!_电影院,前任3,刘若英,张一白,田壮壮\r\n",
      "6552472345548685837_!_102_!_news_entertainment_!_超级英雄演员颜值身材排名，钢铁侠进不了前5，第一名很意外_!_金刚狼3,休·杰克曼,神奇女侠,绯红女巫,超人,金刚狼\r\n",
      "6552385284682547716_!_102_!_news_entertainment_!_《无限歌谣季》热播 张绍刚毛不易组合似“父子”_!_张绍刚,新组合,腾讯视频,无限歌谣季,毛不易,父子\r\n",
      "6552364464715334151_!_102_!_news_entertainment_!_张靓颖透露右耳已经间歇性失聪10年，这些年她都是怎么过来的啊_!_中岛美嘉,滨崎步,张靓颖,演唱会,林子祥\r\n",
      "6552310157706002702_!_102_!_news_entertainment_!_成龙改口决定不裸捐了，20亿财产给儿子一半，你怎么看？_!_\r\n",
      "6552286735408038403_!_102_!_news_entertainment_!_五大“出轨“女明星，最后一个你们肯定不知道！_!_王全安,张柏芝,张雨绮,吴卓林,谢霆锋\r\n",
      "6552269871697101315_!_102_!_news_entertainment_!_认真搞笑的男人最帅！大张伟不张嘴就是美男子之最！蜜汁好看~_!_天天向上,毒鸡汤,大张伟,美男子,综艺,我去上学了,百变大咖秀\r\n",
      "6552418723179790856_!_102_!_news_entertainment_!_谢娜三喜临门，何炅送祝福，吴昕送祝福，只有沈梦辰不一样_!_杜海涛,谢娜,何炅,沈梦辰,吴昕,快本\r\n",
      "6552283654494617859_!_102_!_news_entertainment_!_如何评价赵丽颖？_!_\r\n",
      "6552453686398812423_!_102_!_news_entertainment_!_有哪些偏冷门的歌曲推荐？_!_\r\n",
      "6552383324696871427_!_102_!_news_entertainment_!_“整容狂人”的审美，恕欣赏不来_!_高富帅,阿尔维斯,颜值,吴彦祖,罗德里戈\r\n",
      "6552390851157295629_!_102_!_news_entertainment_!_杨幂景甜徐冬冬唐嫣 不好好穿衣却美的有趣又撩人_!_杨幂,徐冬冬,背带裙,大唐荣耀,唐嫣,景甜\r\n",
      "6552445894711575043_!_102_!_news_entertainment_!_吴卓林：你父母固然有责任，但最大的责任还是在于你自己！_!_孤儿院,吴卓林,小龙女,加拿大\r\n",
      "6552351831668818435_!_102_!_news_entertainment_!_《天乩战之白蛇传说》赵雅芝和杨紫饰演母女会擦出什么样的火花？_!_欢乐颂,西王母,老戏骨,赵雅芝,杨紫,刘嘉玲,新白娘子传奇\r\n",
      "6552474018920792580_!_102_!_news_entertainment_!_他是最帅反派专业户，演《古惑仔》大火，今病魔缠身可怜无人识！_!_浩南哥,黄秋生,龙虎风云,东陵大盗,爱恋狂潮,李子雄,锣鼓巷,古惑仔,学校风云,郑伊健,张耀扬,监狱风云,无间道2\r\n",
      "6552309039697494532_!_103_!_news_sports_!_亚洲杯夺冠赔率：日本、伊朗领衔 中国竟与泰国并列_!_土库曼斯坦,乌兹别克斯坦,亚洲杯,赔率,小组赛\r\n",
      "6552477789642031623_!_103_!_news_sports_!_9轮4球本土射手仅次武磊 黄紫昌要抢最强U23头衔_!_黄紫昌,武磊,卡佩罗,惠家康,韦世豪\r\n",
      "6552495859798376712_!_103_!_news_sports_!_如果今年勇士夺冠，下赛季詹姆斯何去何从？_!_\r\n",
      "6552202204621570574_!_103_!_news_sports_!_超级替补！科斯塔本赛季替补出场贡献7次助攻_!_替补出场,助攻,科斯塔,赫迪拉,博洛尼亚\r\n",
      "6552409639911162381_!_103_!_news_sports_!_骑士6天里发生了啥？从首轮抢七到次轮3-0猛龙_!_骑士,半决赛,猛龙,德罗赞,乐福\r\n",
      "6552470008188895491_!_103_!_news_sports_!_如果朗多进入转会市场，哪些球队适合他？_!_\r\n",
      "6552316209847599367_!_103_!_news_sports_!_詹姆斯G3决杀，你怎么看？_!_\r\n",
      "6552295807171691022_!_103_!_news_sports_!_大魔王带头唱歌！火箭这像是打季后赛？爵士神帅这话已提前投降了_!_PJ-塔克,斯奈德,大魔王,火箭,爵士\r\n",
      "6552452056043487748_!_103_!_news_sports_!_马夏尔要去切尔西？可以商量，不过穆里尼奥的要价是4000万加威廉_!_威廉,曼联,穆里尼奥,布莱顿,马夏尔\r\n",
      "6552287279061139972_!_103_!_news_sports_!_利希施泰纳宣布赛季结束后离队：我需要新的挑战_!_尤文,博洛尼亚,施泰纳,利希施泰纳,拉齐奥\r\n",
      "6552325997813825805_!_103_!_news_sports_!_怎么样看待大连一方在中超联赛第九轮取得的赛季首胜？_!_\r\n",
      "6552468503452975630_!_103_!_news_sports_!_科勒·卡戴珊与男友TT共进午餐，曾在他怀孕期间偷腥的渣男被原_!_NBA,科勒,卡戴珊,四季酒店,克利夫兰骑士队\r\n",
      "6552312104492204301_!_103_!_news_sports_!_作为央视体育体育频道，CCTV5一到周末就直播马拉松你怎么看？_!_\r\n",
      "6552332687678374151_!_103_!_news_sports_!_如果2018骑士夺冠，詹姆斯这个冠军的含金量有多大？_!_\r\n",
      "6552466727995703815_!_103_!_news_sports_!_昔日中超金靴半场独造6球虐爆辽足 华夏送走他后悔吗？_!_阿洛,阿洛伊西奥,华夏幸福,埃尔纳内斯,穆里奇\r\n",
      "6552416247298916612_!_103_!_news_sports_!_NBA历史排名前十都有谁？_!_\r\n",
      "6552354211424633352_!_103_!_news_sports_!_你希望利物浦赢得欧冠吗？巴萨主帅巴尔韦德的回答耐人寻味_!_双冠王,欧冠赛场,贝里索,巴尔韦德,欧冠冠军\r\n",
      "6552314862163198471_!_103_!_news_sports_!_绝杀！詹姆斯38+7再创传奇一刻，两护法创另类神迹更功不可没_!_猛龙,詹姆斯,伊巴卡,德罗赞,科沃尔\r\n",
      "6552304431164031502_!_103_!_news_sports_!_再现绝杀！今天的老詹怎么吹_!_德罗赞,詹姆斯\r\n",
      "6552404265724281357_!_103_!_news_sports_!_拜仁3比1逆转科隆，J罗现世界级做饼_!_J罗,科隆,拜仁,假动作,托利索\r\n",
      "6552375446955098627_!_104_!_news_finance_!_法治聚焦｜多部门联合发文规范民间借贷，严厉打击暴力催收_!_金融,民间借贷,法治\r\n",
      "6552249357389791758_!_104_!_news_finance_!_中国最独特的公司，温氏究竟厉害在哪里？_!_互联网企业,养猪,分布式,家庭农场,新兴县,温氏集团,温北英,可口可乐,技术创新,周其仁,农业\r\n",
      "6552392199802192391_!_104_!_news_finance_!_投P2P本金加12%利息全打水漂 后来我开始研究贷款_!_信用卡,股票,银行,P2P,利息,余额宝\r\n",
      "6552423505156112899_!_104_!_news_finance_!_5月31号前，企业需完成汇算清缴工作，会计注意这5个细节问题_!_汇算清缴,纳税人,残疾人,汇总纳税企业,企业所得税税款\r\n",
      "6552393713924964872_!_104_!_news_finance_!_下周一（5.7日）手上持有这些股的要小心_!_股权解禁,新五丰,鲁阳节能,解禁,大小非\r\n",
      "6552284551496859908_!_104_!_news_finance_!_5万元资金，该做什么行业？_!_\r\n",
      "6552465616924574222_!_104_!_news_finance_!_信用卡账单免息期与最长50天免息还款技巧大全！_!_还款日,账单日,还款,信用卡,免息期\r\n",
      "6552379453605937422_!_104_!_news_finance_!_美增加关税，为何人民币不降反升？_!_\r\n",
      "6552381513936142862_!_104_!_news_finance_!_冯天睿：5.6千三大关何以告破 黄金原油下周一操作建议_!_心静如水,大阳线,MACD,操作,黄金,原油\r\n",
      "6552128165496488456_!_104_!_news_finance_!_报名｜界面资本论坛热点前瞻：各界共议“创造可持续价值”_!_海兰信,安永,中国企业,上海电气,一带一路,交通银行,创造可持续价值,上市,华建集团,阿里巴巴,国药控股\r\n",
      "6552367589337596174_!_104_!_news_finance_!_2017年的巴菲特股东大会有哪些看点？_!_\r\n",
      "6552171081761817091_!_104_!_news_finance_!_巴菲特：美国人生活会越来越好 比洛克菲勒都要过得好_!_美国,金融界,巴菲特,民主党,洛克菲勒\r\n",
      "6552135862929326600_!_104_!_news_finance_!_践行“军民融合”，助力“一带一路”_!_中国电子信息产业集团,电子对抗,美亚柏科,雷科防务,华力创通,中国电子,旋极信息,军民融合,民营企业,中国电子科技集团公司\r\n",
      "6552348488141636099_!_104_!_news_finance_!_重磅：“5050”计划打造全国人才生态最优区 “国际滨”蓄势待发_!_5050计划,高新区,创新创业,阿里巴巴,滨江,杭州市\r\n",
      "6552493681381736963_!_104_!_news_finance_!_不仅首季业绩持续高增长，这些股中期业绩仍可期_!_季度业绩,业绩预告,华帝股份,上证指数,净利润增幅,业绩增长,高增长,有色金属\r\n",
      "6552479093923774734_!_104_!_news_finance_!_目前有35万存款，怎样在短期之内翻倍？_!_\r\n",
      "6552387539792036360_!_104_!_news_finance_!_中兴通讯：关于重大事项进展公告_!_中兴通讯,BIS,中兴通讯股份有限公司\r\n",
      "6552474712801280520_!_104_!_news_finance_!_好好把握比特币的调整 每次都是机会_!_巴菲特,虚拟货币,伯克希尔哈撒韦公司,伯克希尔哈撒韦,比特币\r\n",
      "6552356541591192067_!_104_!_news_finance_!_寻找地产界“最强大脑”——中国房企2018年度CFO评选启幕_!_中国房地产,最强大脑,超级CFO,房地产行业,房地产\r\n",
      "6552451858437243395_!_104_!_news_finance_!_独角兽药明康德周二上市，但一季报净利已现业绩下滑_!_巨人网络,药明康德,分众传媒,招股书,独角兽企业\r\n",
      "6552303945463628301_!_106_!_news_house_!_杭州哪里还有“1万+”的新房？优先选择哪一个？_!_摇号,未来科技城,阳光城,小高层,酒店式公寓\r\n",
      "6552357571120857607_!_106_!_news_house_!_三明贵溪洋拟建服务型公寓，共300多套！附市区5月份二手房价_!_徐碧街道,宏图花园,城乡规划局,村民委员会,二手房\r\n",
      "6552472831328780803_!_106_!_news_house_!_互联侃谈：霍官泰和他的楼花，房地产的行业，到底有多深？_!_期房,何鸿燊,分层产权制度,霍官泰,楼花\r\n",
      "6552418506741121544_!_106_!_news_house_!_山东最新17市的房价出炉了！快看，你家房价的走向！是真高！_!_房价,山东省第一季度薪酬表,二手房价格行政区二手房,山东,二手房\r\n",
      "6552413070562427144_!_106_!_news_house_!_物业公司的服务水准非常烂，该怎么解决？_!_\r\n",
      "6552300361216950531_!_106_!_news_house_!_实行房产税和遗产税之后小产权房怎么办？_!_\r\n",
      "6552327554336817678_!_106_!_news_house_!_成都摇号选房·天立世纪华府——彭州清水均价5476，不是投资处_!_都江堰,彭州,楼盘,天立世纪华府,㎡三室两厅两卫户型\r\n",
      "6552296976585589251_!_106_!_news_house_!_「头条」2018-金色里程，最新详情解析！马鞍山当涂住宅！_!_当涂,城市发展规划,金色里程,马鞍山,当涂县,马鞍山市\r\n",
      "6552301698050687501_!_106_!_news_house_!_瞭望刊文：房地产税改革渐行渐近 还需突破四个阻力区_!_税改革,土地财政,产权房,胡光辉,瞭望,不动产,房地产税,税费\r\n",
      "6552494403947069704_!_106_!_news_house_!_四川巴中有哪些好的装修队？_!_\r\n",
      "6551907587439198723_!_106_!_news_house_!_房屋被征收方强拆，已过起诉时效，还有什么救济方法？_!_行政行为,诉讼时效,行政机关\r\n",
      "6552272429509509389_!_106_!_news_house_!_惠州惠阳13000一平的房子值得买吗？_!_\r\n",
      "6552403967991611662_!_106_!_news_house_!_铜梁的房价是多少？_!_\r\n",
      "6552316609770291726_!_106_!_news_house_!_售楼闺蜜告诉我，这几种户型的房子万万别买，后悔我家买错了_!_房价,户型,售楼,买房\r\n",
      "6552397419261198606_!_106_!_news_house_!_未来房价会跌到什么程度？_!_\r\n",
      "6552387849344254222_!_106_!_news_house_!_未来房价会跌到什么程度？_!_\r\n",
      "6552295326462509575_!_106_!_news_house_!_2018年的房价回是什么趋势？_!_房地产泡沫,海南岛,房价,大洗牌,房地产开发商\r\n",
      "6552327314498126083_!_106_!_news_house_!_五月份是买房的好时间吗？_!_\r\n",
      "6552114549741322766_!_106_!_news_house_!_火爆的楼市现状，刚需最后的上车机会？_!_保利新武昌,首套房,购房者,购房合同,利率上浮\r\n",
      "6552236298344595982_!_106_!_news_house_!_青岛这个区牛了！要建奥特莱斯、银座……还通地铁！_!_世茂,银座,高新区,奥特莱斯,青岛高新区,项目投资协议\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 100 /home/ec2-user/SageMaker/chinese-corpus/toutiao_cat_data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat /home/ec2-user/SageMaker/chinese-corpus/toutiao_cat_data.txt | grep '_news_entertainment_' > toutiao_cat_data_ent.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can download the data. *Please review the following Acknowledgements, Copyright Information, and Availability notice before downloading the data.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## From Plain Text to Bag-of-Words (BOW)\n",
    "\n",
    "The input documents to the algorithm, both in training and inference, need to be vectors of integers representing word counts. This is so-called bag-of-words (BOW) representation. To convert plain text to BOW, we need to first \"tokenize\" our documents, i.e identify words and assign an integer id to each of them.\n",
    "\n",
    "$$\n",
    "\\text{\"cat\"} \\mapsto 0, \\; \\text{\"dog\"} \\mapsto 1 \\; \\text{\"bird\"} \\mapsto 2, \\ldots\n",
    "$$\n",
    "\n",
    "Then, we count the occcurence of each of the tokens in each document and form BOW vectors as illustrated in the following example:\n",
    "\n",
    "$$\n",
    "w = \\text{\"cat bird bird bird cat\"} \\quad \\longmapsto \\quad w = [2, 0, 3, 0, \\ldots, 0]\n",
    "$$\n",
    "\n",
    "Also, note that many real-world applications have large vocabulary sizes. It may be necessary to represent the input documents in sparse format. Finally, the use of stemming and lemmatization in data preprocessing provides several benefits. Doing so can improve training and inference compute time since it reduces the effective vocabulary size. More importantly, though, it can improve the quality of learned topic-word probability matrices and inferred topic mixtures. For example, the words *\"parliament\"*, *\"parliaments\"*, *\"parliamentary\"*, *\"parliament's\"*, and *\"parliamentarians\"* are all essentially the same word, *\"parliament\"*, but with different conjugations. For the purposes of detecting topics, such as a *\"politics\"* or *governments\"* topic, the inclusion of all five does not add much additional value as they all essentially describe the same feature.\n",
    "\n",
    "In this example, we will use `CountVectorizer` in `scikit-learn` to perform the token counting. For more details please refer to their documentation respectively. Alternatively, [`spaCy`](https://spacy.io/) also offers easy-to-use tokenization and lemmatization functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /home/ec2-user/.cache/pip/wheels/17/a7/8b/a7e03881534e78558920ac68aaeca05180c0e2c3d11c4fce3b/jieba-0.42.1-py3-none-any.whl\n",
      "Installing collected packages: jieba\n",
      "Successfully installed jieba-0.42.1\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/mxnet_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting ckiptagger\n",
      "  Using cached ckiptagger-0.1.1-py3-none-any.whl (34 kB)\n",
      "Installing collected packages: ckiptagger\n",
      "Successfully installed ckiptagger-0.1.1\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/mxnet_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting tensorflow==1.13.1\n",
      "  Using cached tensorflow-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (92.5 MB)\n",
      "Collecting gast>=0.2.0\n",
      "  Using cached gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from tensorflow==1.13.1) (1.1.2)\n",
      "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
      "  Using cached tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367 kB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from tensorflow==1.13.1) (1.18.1)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from tensorflow==1.13.1) (3.12.2)\n",
      "Collecting tensorboard<1.14.0,>=1.13.0\n",
      "  Using cached tensorboard-1.13.1-py3-none-any.whl (3.2 MB)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from tensorflow==1.13.1) (1.14.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from tensorflow==1.13.1) (0.34.2)\n",
      "Processing /home/ec2-user/.cache/pip/wheels/c3/af/84/3962a6af7b4ab336e951b7877dcfb758cf94548bb1771e0679/absl_py-0.9.0-py3-none-any.whl\n",
      "Collecting grpcio>=1.8.6\n",
      "  Using cached grpcio-1.30.0-cp36-cp36m-manylinux2010_x86_64.whl (3.0 MB)\n",
      "Processing /home/ec2-user/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc/termcolor-1.1.0-py3-none-any.whl\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from tensorflow==1.13.1) (1.0.8)\n",
      "Collecting astor>=0.6.0\n",
      "  Using cached astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: mock>=2.0.0 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow==1.13.1) (4.0.1)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (46.1.3.post20200330)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (1.0.1)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.2.2-py3-none-any.whl (88 kB)\n",
      "Requirement already satisfied: h5py in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.10.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (1.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (2.2.0)\n",
      "Installing collected packages: gast, absl-py, tensorflow-estimator, markdown, grpcio, tensorboard, termcolor, astor, tensorflow\n",
      "Successfully installed absl-py-0.9.0 astor-0.8.1 gast-0.3.3 grpcio-1.30.0 markdown-3.2.2 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0 termcolor-1.1.0\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/mxnet_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (1.18.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/mxnet_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install jieba\n",
    "!pip install ckiptagger\n",
    "!pip install tensorflow==1.13.1\n",
    "!pip install numpy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the tokenizer defined we perform token counting next while limiting the vocabulary size to `vocab_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.828 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "import numpy as np \n",
    "\n",
    "max_feature_dim = 5000 \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import sys\n",
    "sys.path.append('/home/ec2-user/SageMaker/nlp_processing/')\n",
    "\n",
    "from preprocess import TouTiaoNewsPreprocessor\n",
    "from segmenter import JiebaSegmenter\n",
    "from transformer import BlazingTextInputDataTransformer        \n",
    "        \n",
    "\n",
    "\n",
    "def to_sparse_vectors(documents, max_feature_dim): \n",
    "    vectorizer = CountVectorizer(input='content', analyzer='word', stop_words='english',max_features=max_feature_dim, max_df=0.95, min_df=2)\n",
    "    vectors = vectorizer.fit_transform(documents)\n",
    "    vocab_list = vectorizer.get_feature_names()\n",
    "    return vectors, vocab_list\n",
    "\n",
    "            \n",
    "\n",
    "preprocessor = TouTiaoNewsPreprocessor()\n",
    "res = preprocessor.preprocess('toutiao_cat_data_ent.txt')\n",
    "segmenter = JiebaSegmenter()\n",
    "transformer = BlazingTextInputDataTransformer()\n",
    "document_str = [] \n",
    "for r in res:\n",
    "    toks = segmenter.segment(r[2])\n",
    "    document_str.append(' '.join(toks))    \n",
    "    \n",
    "\n",
    "\n",
    "vectors, vocab_list = to_sparse_vectors(document_str, max_feature_dim)\n",
    "vocabulary_size = len(vocab_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output from `CountVectorizer` are sparse matrices with their elements being integers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'> int64\n",
      "  (0, 4323)\t1\n",
      "  (0, 2896)\t1\n",
      "  (0, 3257)\t1\n",
      "  (0, 3903)\t1\n",
      "  (0, 475)\t1\n",
      "  (0, 422)\t1\n",
      "  (0, 4184)\t1\n",
      "  (0, 4052)\t1\n"
     ]
    }
   ],
   "source": [
    "print(type(vectors), vectors.dtype)\n",
    "print(vectors[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because all the parameters (weights and biases) in the NTM model are `np.float32` type we'd need the input data to also be in `np.float32`. It is better to do this type-casting upfront rather than repeatedly casting during mini-batch training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'> float32\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse as sparse\n",
    "vectors = sparse.csr_matrix(vectors, dtype=np.float32)\n",
    "print(type(vectors), vectors.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a common practice in modeling training, we should have a training set, a validation set, and a test set. The training set is the set of data the model is actually being trained on. But what we really care about is not the model's performance on training set but its performance on future, unseen data. Therefore, during training, we periodically calculate scores (or losses) on the validation set to validate the performance of the model on unseen data. By assessing the model's ability to generalize we can stop the training at the optimal point via early stopping to avoid over-training. \n",
    "\n",
    "Note that when we only have a training set and no validation set, the NTM model will rely on scores on the training set to perform early stopping, which could result in over-training. Therefore, we recommend always supply a validation set to the model.\n",
    "\n",
    "Here we use 80% of the data set as the training set and the rest for validation set and test set. We will use the validation set in training and use the test set for demonstrating model inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = int(0.8 * vectors.shape[0])\n",
    "\n",
    "# split train and test\n",
    "train_vectors = vectors[0:n_train, :]\n",
    "test_vectors = vectors[n_train:, :]\n",
    "\n",
    "# further split test set into validation set (val_vectors) and test  set (test_vectors)\n",
    "n_test = test_vectors.shape[0]\n",
    "val_vectors = test_vectors[:n_test//2, :]\n",
    "test_vectors = test_vectors[n_test//2:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31516, 5000) (3940, 5000) (3940, 5000)\n"
     ]
    }
   ],
   "source": [
    "print(train_vectors.shape, test_vectors.shape, val_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Store Data on S3\n",
    "\n",
    "The NTM algorithm, as well as other first-party SageMaker algorithms, accepts data in [RecordIO](https://mxnet.apache.org/api/python/io/io.html#module-mxnet.recordio) [Protobuf](https://developers.google.com/protocol-buffers/) format. The SageMaker Python API provides helper functions for easily converting your data into this format. Below we convert the from numpy/scipy data and upload it to an Amazon S3 destination for the model to access it during training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup AWS Credentials\n",
    "\n",
    "We first need to specify data locations and access roles. ***This is the only cell of this notebook that you will need to edit.*** In particular, we need the following data:\n",
    "\n",
    "- The S3 `bucket` and `prefix` that you want to use for training and model data.  This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM `role` is used to give training and hosting access to your data. See the documentation for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the boto regexp with a the appropriate full IAM role arn string(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "bucket = 'cht-ws-nlp-yianc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set location s3://cht-ws-nlp-yianc/ntm/entertainment_news/train\n",
      "Validation set location s3://cht-ws-nlp-yianc/ntm/entertainment_news/val\n",
      "Trained model will be saved at s3://cht-ws-nlp-yianc/ntm/entertainment_news/output\n"
     ]
    }
   ],
   "source": [
    "prefix = 'ntm/entertainment_news'\n",
    "\n",
    "train_prefix = os.path.join(prefix, 'train')\n",
    "val_prefix = os.path.join(prefix, 'val')\n",
    "output_prefix = os.path.join(prefix, 'output')\n",
    "\n",
    "s3_train_data = os.path.join('s3://', bucket, train_prefix)\n",
    "s3_val_data = os.path.join('s3://', bucket, val_prefix)\n",
    "output_path = os.path.join('s3://', bucket, output_prefix)\n",
    "print('Training set location', s3_train_data)\n",
    "print('Validation set location', s3_val_data)\n",
    "print('Trained model will be saved at', output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a helper function to convert the data to RecordIO Protobuf format and upload it to S3. In addition, we will have the option to split the data into several parts specified by `n_parts`.\n",
    "\n",
    "The algorithm inherently supports multiple files in the training folder (\"channel\"), which could be very helpful for large data set. In addition, when we use distributed training with multiple workers (compute instances), having multiple files allows us to distribute different portions of the training data to different workers conveniently.\n",
    "\n",
    "Inside this helper function we use `write_spmatrix_to_sparse_tensor` function provided by [SageMaker Python SDK](https://github.com/aws/sagemaker-python-sdk) to convert scipy sparse matrix into RecordIO Protobuf format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_convert_upload(sparray, bucket, prefix, fname_template='data_part{}.pbr', n_parts=2):\n",
    "    import io\n",
    "    import boto3\n",
    "    import sagemaker.amazon.common as smac\n",
    "    \n",
    "    chunk_size = sparray.shape[0]// n_parts\n",
    "    for i in range(n_parts):\n",
    "\n",
    "        # Calculate start and end indices\n",
    "        start = i*chunk_size\n",
    "        end = (i+1)*chunk_size\n",
    "        if i+1 == n_parts:\n",
    "            end = sparray.shape[0]\n",
    "        \n",
    "        # Convert to record protobuf\n",
    "        buf = io.BytesIO()\n",
    "        smac.write_spmatrix_to_sparse_tensor(array=sparray[start:end], file=buf, labels=None)\n",
    "        buf.seek(0)\n",
    "        \n",
    "        # Upload to s3 location specified by bucket and prefix\n",
    "        fname = os.path.join(prefix, fname_template.format(i))\n",
    "        boto3.resource('s3').Bucket(bucket).Object(fname).upload_fileobj(buf)\n",
    "        print('Uploaded data to s3://{}'.format(os.path.join(bucket, fname)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded data to s3://cht-ws-nlp-yianc/ntm/entertainment_news/train/train_part0.pbr\n",
      "Uploaded data to s3://cht-ws-nlp-yianc/ntm/entertainment_news/train/train_part1.pbr\n",
      "Uploaded data to s3://cht-ws-nlp-yianc/ntm/entertainment_news/train/train_part2.pbr\n",
      "Uploaded data to s3://cht-ws-nlp-yianc/ntm/entertainment_news/train/train_part3.pbr\n",
      "Uploaded data to s3://cht-ws-nlp-yianc/ntm/entertainment_news/train/train_part4.pbr\n",
      "Uploaded data to s3://cht-ws-nlp-yianc/ntm/entertainment_news/train/train_part5.pbr\n",
      "Uploaded data to s3://cht-ws-nlp-yianc/ntm/entertainment_news/train/train_part6.pbr\n",
      "Uploaded data to s3://cht-ws-nlp-yianc/ntm/entertainment_news/train/train_part7.pbr\n",
      "Uploaded data to s3://cht-ws-nlp-yianc/ntm/entertainment_news/val/val_part0.pbr\n"
     ]
    }
   ],
   "source": [
    "split_convert_upload(train_vectors, bucket=bucket, prefix=train_prefix, fname_template='train_part{}.pbr', n_parts=8)\n",
    "split_convert_upload(val_vectors, bucket=bucket, prefix=val_prefix, fname_template='val_part{}.pbr', n_parts=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "# Model Training\n",
    "\n",
    "We have created the training and validation data sets and uploaded them to S3. Next, we configure a SageMaker training job to use the NTM algorithm on the data we prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker uses Amazon Elastic Container Registry (ECR) docker container to host the NTM training image. The following ECR containers are currently available for SageMaker NTM training in different regions. For the latest Docker container registry please refer to [Amazon SageMaker: Common Parameters](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'get_image_uri' method will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(boto3.Session().region_name, 'ntm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below automatically chooses an algorithm container based on the current region. In the API call to `sagemaker.estimator.Estimator` we also specify the type and count of instances for the training job. Because the 20NewsGroups data set is relatively small, we have chosen a CPU only instance (`ml.c4.xlarge`), but do feel free to change to [other instance types](https://aws.amazon.com/sagemaker/pricing/instance-types/). NTM fully takes advantage of GPU hardware and in general trains roughly an order of magnitude faster on a GPU than on a CPU. Multi-GPU or multi-instance training further improves training speed roughly linearly if communication overhead is low compared to compute time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "sess = sagemaker.Session()\n",
    "ntm = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=2, \n",
    "                                    train_instance_type='ml.c4.xlarge',\n",
    "                                    output_path=output_path,\n",
    "                                    sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Here we highlight a few hyperparameters. For information about the full list of available hyperparameters, please refer to [NTM Hyperparameters](https://docs.aws.amazon.com/sagemaker/latest/dg/ntm_hyperparameters.html).\n",
    "\n",
    "- **feature_dim** - the \"feature dimension\", it should be set to the vocabulary size\n",
    "- **num_topics** - the number of topics to extract\n",
    "- **mini_batch_size** - this is the batch size for each worker instance. Note that in multi-GPU instances, this number will be further divided by the number of GPUs. Therefore, for example, if we plan to train on an 8-GPU machine (such as `ml.p2.8xlarge`) and wish each GPU to have 1024 training examples per batch, `mini_batch_size` should be set to 8196.\n",
    "- **epochs** - the maximal number of epochs to train for, training may stop early\n",
    "- **num_patience_epochs** and **tolerance** controls the early stopping behavior. Roughly speaking, the algorithm will stop training if within the last `num_patience_epochs` epochs there have not been improvements on validation loss. Improvements smaller than `tolerance` will be considered non-improvement.\n",
    "- **optimizer** and **learning_rate** - by default we use `adadelta` optimizer and `learning_rate` does not need to be set. For other optimizers, the choice of an appropriate learning rate may require experimentation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_topics = 20\n",
    "\n",
    "ntm.set_hyperparameters(num_topics=num_topics, feature_dim=vocabulary_size, mini_batch_size=128, \n",
    "                        epochs=100, num_patience_epochs=5, tolerance=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to specify how the training data and validation data will be distributed to the workers during training. There are two modes for data channels:\n",
    "\n",
    "- `FullyReplicated`: all data files will be copied to all workers\n",
    "- `ShardedByS3Key`: data files will be sharded to different workers, i.e. each worker will receive a different portion of the full data set.\n",
    "\n",
    "At the time of writing, by default, the Python SDK will use `FullyReplicated` mode for all data channels. This is desirable for validation (test) channel but not suitable for training channel. The reason is that when we use multiple workers we would like to go through the full data set by each of them going through a different portion of the data set, so as to provide different gradients within epochs. Using `FullyReplicated` mode on training data not only results in slower training time per epoch (nearly 1.5X in this example), but also defeats the purpose of distributed training. To set the training data channel correctly we specify `distribution` to be `ShardedByS3Key` for the training data channel as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.session import s3_input\n",
    "s3_train = s3_input(s3_train_data, distribution='ShardedByS3Key') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train. The following cell takes a few minutes to run. The command below will first provision the required hardware. You will see a series of dots indicating the progress of the hardware provisioning process. Once the resources are allocated, training logs will be displayed. With multiple workers, the log color and the ID following `INFO` identifies logs emitted by different workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-14 13:43:37 Starting - Starting the training job...\n",
      "2020-07-14 13:43:39 Starting - Launching requested ML instances.........\n",
      "2020-07-14 13:45:14 Starting - Preparing the instances for training.........\n",
      "2020-07-14 13:46:54 Downloading - Downloading input data\n",
      "2020-07-14 13:46:54 Training - Downloading the training image..\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[35mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[35mRunning default environment configuration script\u001b[0m\n",
      "\u001b[35m/opt/amazon/lib/python2.7/site-packages/pandas/util/nosetester.py:13: DeprecationWarning: Importing from numpy.testing.nosetester is deprecated, import from numpy.testing instead.\n",
      "  from numpy.testing import nosetester\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:15 INFO 140442437990208] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/default-input.json: {u'num_patience_epochs': u'3', u'clip_gradient': u'Inf', u'encoder_layers': u'auto', u'optimizer': u'adadelta', u'_kvstore': u'auto_gpu', u'rescale_gradient': u'1.0', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'learning_rate': u'0.01', u'_data_format': u'record', u'sub_sample': u'1.0', u'epochs': u'50', u'weight_decay': u'0.0', u'_num_kv_servers': u'auto', u'encoder_layers_activation': u'sigmoid', u'mini_batch_size': u'256', u'tolerance': u'0.001', u'batch_norm': u'false'}\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:15 INFO 140442437990208] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'num_patience_epochs': u'5', u'num_topics': u'20', u'epochs': u'100', u'feature_dim': u'5000', u'mini_batch_size': u'128', u'tolerance': u'0.001'}\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:15 INFO 140442437990208] Final configuration: {u'optimizer': u'adadelta', u'rescale_gradient': u'1.0', u'_tuning_objective_metric': u'', u'learning_rate': u'0.01', u'clip_gradient': u'Inf', u'feature_dim': u'5000', u'encoder_layers_activation': u'sigmoid', u'_num_kv_servers': u'auto', u'weight_decay': u'0.0', u'num_patience_epochs': u'5', u'epochs': u'100', u'mini_batch_size': u'128', u'num_topics': u'20', u'_num_gpus': u'auto', u'_data_format': u'record', u'sub_sample': u'1.0', u'_kvstore': u'auto_gpu', u'encoder_layers': u'auto', u'tolerance': u'0.001', u'batch_norm': u'false'}\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:15 INFO 140442437990208] nvidia-smi took: 0.025171995163 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:15 INFO 140442437990208] Launching parameter server for role server\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:15 INFO 140442437990208] {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/7ccf53e6-c6ae-4788-af12-30cb078c0cad', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/7ccf53e6-c6ae-4788-af12-30cb078c0cad', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'AWS_REGION': 'us-west-2', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'ntm-2020-07-14-13-43-37-615', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'ip-10-0-179-76.us-west-2.compute.internal', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/bb19fc2b-25a2-43cc-a5fd-98fdb1abcbb2', 'PWD': '/', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:702018522796:training-job/ntm-2020-07-14-13-43-37-615', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:15 INFO 140442437990208] envs={'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/7ccf53e6-c6ae-4788-af12-30cb078c0cad', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/7ccf53e6-c6ae-4788-af12-30cb078c0cad', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_NUM_WORKER': '2', 'DMLC_PS_ROOT_PORT': '9000', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.0.155.96', 'AWS_REGION': 'us-west-2', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'ntm-2020-07-14-13-43-37-615', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'ip-10-0-179-76.us-west-2.compute.internal', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/bb19fc2b-25a2-43cc-a5fd-98fdb1abcbb2', 'DMLC_ROLE': 'server', 'PWD': '/', 'DMLC_NUM_SERVER': '2', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:702018522796:training-job/ntm-2020-07-14-13-43-37-615', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:15 INFO 140442437990208] Environment: {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/7ccf53e6-c6ae-4788-af12-30cb078c0cad', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/7ccf53e6-c6ae-4788-af12-30cb078c0cad', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_WORKER': '2', 'SAGEMAKER_HTTP_PORT': '8080', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.0.155.96', 'AWS_REGION': 'us-west-2', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'ntm-2020-07-14-13-43-37-615', 'HOME': '/root', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'ip-10-0-179-76.us-west-2.compute.internal', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/bb19fc2b-25a2-43cc-a5fd-98fdb1abcbb2', 'DMLC_ROLE': 'worker', 'PWD': '/', 'DMLC_NUM_SERVER': '2', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:702018522796:training-job/ntm-2020-07-14-13-43-37-615', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[35mProcess 36 is a shell:server.\u001b[0m\n",
      "\u001b[35mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:15 INFO 140442437990208] Using default worker.\u001b[0m\n",
      "\u001b[35m[2020-07-14 13:47:15.992] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:15 INFO 140442437990208] Initializing\u001b[0m\n",
      "\u001b[35m/opt/amazon/lib/python2.7/site-packages/ai_algorithms_sdk/config/config_helper.py:122: DeprecationWarning: deprecated\n",
      "  warnings.warn(\"deprecated\", DeprecationWarning)\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:15 INFO 140442437990208] None\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:15 INFO 140442437990208] vocab.txt\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:15 INFO 140442437990208] Vocab file is not provided\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:16 INFO 140442437990208] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:16 INFO 140442437990208] Create Store: dist_async\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python2.7/site-packages/pandas/util/nosetester.py:13: DeprecationWarning: Importing from numpy.testing.nosetester is deprecated, import from numpy.testing instead.\n",
      "  from numpy.testing import nosetester\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:17 INFO 139835197114176] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/default-input.json: {u'num_patience_epochs': u'3', u'clip_gradient': u'Inf', u'encoder_layers': u'auto', u'optimizer': u'adadelta', u'_kvstore': u'auto_gpu', u'rescale_gradient': u'1.0', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'learning_rate': u'0.01', u'_data_format': u'record', u'sub_sample': u'1.0', u'epochs': u'50', u'weight_decay': u'0.0', u'_num_kv_servers': u'auto', u'encoder_layers_activation': u'sigmoid', u'mini_batch_size': u'256', u'tolerance': u'0.001', u'batch_norm': u'false'}\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:17 INFO 139835197114176] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'num_patience_epochs': u'5', u'num_topics': u'20', u'epochs': u'100', u'feature_dim': u'5000', u'mini_batch_size': u'128', u'tolerance': u'0.001'}\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:17 INFO 139835197114176] Final configuration: {u'optimizer': u'adadelta', u'rescale_gradient': u'1.0', u'_tuning_objective_metric': u'', u'learning_rate': u'0.01', u'clip_gradient': u'Inf', u'feature_dim': u'5000', u'encoder_layers_activation': u'sigmoid', u'_num_kv_servers': u'auto', u'weight_decay': u'0.0', u'num_patience_epochs': u'5', u'epochs': u'100', u'mini_batch_size': u'128', u'num_topics': u'20', u'_num_gpus': u'auto', u'_data_format': u'record', u'sub_sample': u'1.0', u'_kvstore': u'auto_gpu', u'encoder_layers': u'auto', u'tolerance': u'0.001', u'batch_norm': u'false'}\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:17 INFO 139835197114176] nvidia-smi took: 0.0251858234406 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:17 INFO 139835197114176] Launching parameter server for role scheduler\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:17 INFO 139835197114176] {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/95ecd368-5fd6-45e2-b9a9-5dedf8c75777', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/95ecd368-5fd6-45e2-b9a9-5dedf8c75777', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'AWS_REGION': 'us-west-2', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'ntm-2020-07-14-13-43-37-615', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'ip-10-0-155-96.us-west-2.compute.internal', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/add8fc5f-5521-4aca-b337-984c949dfc5a', 'PWD': '/', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:702018522796:training-job/ntm-2020-07-14-13-43-37-615', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:17 INFO 139835197114176] envs={'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/95ecd368-5fd6-45e2-b9a9-5dedf8c75777', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/95ecd368-5fd6-45e2-b9a9-5dedf8c75777', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_NUM_WORKER': '2', 'DMLC_PS_ROOT_PORT': '9000', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.0.155.96', 'AWS_REGION': 'us-west-2', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'ntm-2020-07-14-13-43-37-615', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'ip-10-0-155-96.us-west-2.compute.internal', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/add8fc5f-5521-4aca-b337-984c949dfc5a', 'DMLC_ROLE': 'scheduler', 'PWD': '/', 'DMLC_NUM_SERVER': '2', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:702018522796:training-job/ntm-2020-07-14-13-43-37-615', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:17 INFO 139835197114176] Launching parameter server for role server\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:17 INFO 139835197114176] {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/95ecd368-5fd6-45e2-b9a9-5dedf8c75777', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/95ecd368-5fd6-45e2-b9a9-5dedf8c75777', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'AWS_REGION': 'us-west-2', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'ntm-2020-07-14-13-43-37-615', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'ip-10-0-155-96.us-west-2.compute.internal', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/add8fc5f-5521-4aca-b337-984c949dfc5a', 'PWD': '/', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:702018522796:training-job/ntm-2020-07-14-13-43-37-615', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:17 INFO 139835197114176] envs={'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/95ecd368-5fd6-45e2-b9a9-5dedf8c75777', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/95ecd368-5fd6-45e2-b9a9-5dedf8c75777', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_NUM_WORKER': '2', 'DMLC_PS_ROOT_PORT': '9000', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'SAGEMAKER_HTTP_PORT': '8080', 'HOME': '/root', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.0.155.96', 'AWS_REGION': 'us-west-2', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'ntm-2020-07-14-13-43-37-615', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'ip-10-0-155-96.us-west-2.compute.internal', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/add8fc5f-5521-4aca-b337-984c949dfc5a', 'DMLC_ROLE': 'server', 'PWD': '/', 'DMLC_NUM_SERVER': '2', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:702018522796:training-job/ntm-2020-07-14-13-43-37-615', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:17 INFO 139835197114176] Environment: {'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/95ecd368-5fd6-45e2-b9a9-5dedf8c75777', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/95ecd368-5fd6-45e2-b9a9-5dedf8c75777', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION_VERSION': '2', 'DMLC_PS_ROOT_PORT': '9000', 'DMLC_NUM_WORKER': '2', 'SAGEMAKER_HTTP_PORT': '8080', 'PATH': '/opt/amazon/bin:/usr/local/nvidia/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/amazon/bin:/opt/amazon/bin', 'PYTHONUNBUFFERED': 'TRUE', 'CANONICAL_ENVROOT': '/opt/amazon', 'LD_LIBRARY_PATH': '/opt/amazon/lib/python2.7/site-packages/cv2/../../../../lib:/usr/local/nvidia/lib64:/opt/amazon/lib', 'LANG': 'en_US.utf8', 'DMLC_INTERFACE': 'eth0', 'SHLVL': '1', 'DMLC_PS_ROOT_URI': '10.0.155.96', 'AWS_REGION': 'us-west-2', 'SAGEMAKER_METRICS_DIRECTORY': '/opt/ml/output/metrics/sagemaker', 'NVIDIA_VISIBLE_DEVICES': 'void', 'TRAINING_JOB_NAME': 'ntm-2020-07-14-13-43-37-615', 'HOME': '/root', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'cpp', 'ENVROOT': '/opt/amazon', 'SAGEMAKER_DATA_PATH': '/opt/ml', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NVIDIA_REQUIRE_CUDA': 'cuda>=9.0', 'OMP_NUM_THREADS': '2', 'HOSTNAME': 'ip-10-0-155-96.us-west-2.compute.internal', 'MXNET_STORAGE_FALLBACK_LOG_VERBOSE': '0', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/add8fc5f-5521-4aca-b337-984c949dfc5a', 'DMLC_ROLE': 'worker', 'PWD': '/', 'DMLC_NUM_SERVER': '2', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-west-2:702018522796:training-job/ntm-2020-07-14-13-43-37-615', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2'}\u001b[0m\n",
      "\u001b[34mProcess 36 is a shell:scheduler.\u001b[0m\n",
      "\u001b[34mProcess 37 is a shell:server.\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:17 INFO 139835197114176] Using default worker.\u001b[0m\n",
      "\u001b[34m[2020-07-14 13:47:18.054] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:18 INFO 139835197114176] Initializing\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python2.7/site-packages/ai_algorithms_sdk/config/config_helper.py:122: DeprecationWarning: deprecated\n",
      "  warnings.warn(\"deprecated\", DeprecationWarning)\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:18 INFO 139835197114176] None\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:18 INFO 139835197114176] vocab.txt\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:18 INFO 139835197114176] Vocab file is not provided\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:18 INFO 139835197114176] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:18 INFO 139835197114176] Create Store: dist_async\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}}, \"EndTime\": 1594734438.656627, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1594734438.656586}\n",
      "\u001b[0m\n",
      "\u001b[34m[2020-07-14 13:47:18.656] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 621, \"num_examples\": 1, \"num_bytes\": 8728}\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:18 INFO 139835197114176] \u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:18 INFO 139835197114176] # Starting training for epoch 1\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}}, \"EndTime\": 1594734438.656716, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1594734438.656685}\n",
      "\u001b[0m\n",
      "\u001b[35m[2020-07-14 13:47:18.656] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 2670, \"num_examples\": 1, \"num_bytes\": 8292}\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:18 INFO 140442437990208] \u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:18 INFO 140442437990208] # Starting training for epoch 1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2020-07-14 13:47:14 Training - Training image download completed. Training in progress.\u001b[34m[2020-07-14 13:47:25.394] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 6736, \"num_examples\": 124, \"num_bytes\": 1057704}\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:25 INFO 139835197114176] # Finished training epoch 1 on 15756 examples from 124 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:25 INFO 139835197114176] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:25 INFO 139835197114176] Loss (name: value) total: 7.79334108099\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:25 INFO 139835197114176] Loss (name: value) kld: 0.0158516121507\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:25 INFO 139835197114176] Loss (name: value) recons: 7.77748944731\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:25 INFO 139835197114176] Loss (name: value) logppx: 7.79334108099\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:25 INFO 139835197114176] #quality_metric: host=algo-1, epoch=1, train total_loss <loss>=7.79334108099\u001b[0m\n",
      "\u001b[34m[2020-07-14 13:47:25.399] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 0, \"duration\": 7344, \"num_examples\": 1, \"num_bytes\": 8344}\u001b[0m\n",
      "\u001b[34m[2020-07-14 13:47:25.874] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 2, \"duration\": 474, \"num_examples\": 31, \"num_bytes\": 263840}\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:25 INFO 139835197114176] Finished scoring on 3840 examples from 30 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:25 INFO 139835197114176] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:25 INFO 139835197114176] Loss (name: value) total: 7.8657664458\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:25 INFO 139835197114176] Loss (name: value) kld: 0.010422862585\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:25 INFO 139835197114176] Loss (name: value) recons: 7.85534362793\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:25 INFO 139835197114176] Loss (name: value) logppx: 7.8657664458\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:25 INFO 139835197114176] #validation_score (1): 7.865766445795695\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:25 INFO 139835197114176] Timing: train: 6.74s, val: 0.48s, epoch: 7.22s\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:25 INFO 139835197114176] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 124, \"sum\": 124.0, \"min\": 124}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 124, \"sum\": 124.0, \"min\": 124}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 15756, \"sum\": 15756.0, \"min\": 15756}, \"Total Batches Seen\": {\"count\": 1, \"max\": 124, \"sum\": 124.0, \"min\": 124}, \"Total Records Seen\": {\"count\": 1, \"max\": 15756, \"sum\": 15756.0, \"min\": 15756}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 15756, \"sum\": 15756.0, \"min\": 15756}, \"Reset Count\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}}, \"EndTime\": 1594734445.876715, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 0}, \"StartTime\": 1594734438.657029}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:25 INFO 139835197114176] #throughput_metric: host=algo-1, train throughput=2182.31167133 records/second\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:25 INFO 139835197114176] \u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:25 INFO 139835197114176] # Starting training for epoch 2\u001b[0m\n",
      "\u001b[35m[2020-07-14 13:47:25.477] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 6819, \"num_examples\": 124, \"num_bytes\": 1052548}\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:25 INFO 140442437990208] # Finished training epoch 1 on 15760 examples from 124 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:25 INFO 140442437990208] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:25 INFO 140442437990208] Loss (name: value) total: 7.82202113252\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:25 INFO 140442437990208] Loss (name: value) kld: 0.017115066795\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:25 INFO 140442437990208] Loss (name: value) recons: 7.80490611061\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:25 INFO 140442437990208] Loss (name: value) logppx: 7.82202113252\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:25 INFO 140442437990208] #quality_metric: host=algo-2, epoch=1, train total_loss <loss>=7.82202113252\u001b[0m\n",
      "\u001b[35m[2020-07-14 13:47:25.482] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 0, \"duration\": 9490, \"num_examples\": 1, \"num_bytes\": 8344}\u001b[0m\n",
      "\u001b[35m[2020-07-14 13:47:25.954] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 2, \"duration\": 471, \"num_examples\": 31, \"num_bytes\": 263840}\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:25 INFO 140442437990208] Finished scoring on 3840 examples from 30 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:25 INFO 140442437990208] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:25 INFO 140442437990208] Loss (name: value) total: 7.86329067548\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:25 INFO 140442437990208] Loss (name: value) kld: 0.0116105838368\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:25 INFO 140442437990208] Loss (name: value) recons: 7.85168008804\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:25 INFO 140442437990208] Loss (name: value) logppx: 7.86329067548\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:25 INFO 140442437990208] #validation_score (1): 7.863290675481161\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:25 INFO 140442437990208] Timing: train: 6.82s, val: 0.47s, epoch: 7.30s\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:25 INFO 140442437990208] #progress_metric: host=algo-2, completed 1 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 124, \"sum\": 124.0, \"min\": 124}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 124, \"sum\": 124.0, \"min\": 124}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 15760, \"sum\": 15760.0, \"min\": 15760}, \"Total Batches Seen\": {\"count\": 1, \"max\": 124, \"sum\": 124.0, \"min\": 124}, \"Total Records Seen\": {\"count\": 1, \"max\": 15760, \"sum\": 15760.0, \"min\": 15760}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 15760, \"sum\": 15760.0, \"min\": 15760}, \"Reset Count\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}}, \"EndTime\": 1594734445.956643, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 0}, \"StartTime\": 1594734438.65707}\n",
      "\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:25 INFO 140442437990208] #throughput_metric: host=algo-2, train throughput=2158.98394394 records/second\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:25 INFO 140442437990208] \u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:25 INFO 140442437990208] # Starting training for epoch 2\u001b[0m\n",
      "\u001b[35m[2020-07-14 13:47:33.005] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 5, \"duration\": 7047, \"num_examples\": 124, \"num_bytes\": 1052548}\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:33 INFO 140442437990208] # Finished training epoch 2 on 15760 examples from 124 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:33 INFO 140442437990208] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:33 INFO 140442437990208] Loss (name: value) total: 7.75468289756\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:33 INFO 140442437990208] Loss (name: value) kld: 0.0230430440711\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:33 INFO 140442437990208] Loss (name: value) recons: 7.73163983514\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:33 INFO 140442437990208] Loss (name: value) logppx: 7.75468289756\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:33 INFO 140442437990208] #quality_metric: host=algo-2, epoch=2, train total_loss <loss>=7.75468289756\u001b[0m\n",
      "\u001b[35m[2020-07-14 13:47:33.539] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 5, \"duration\": 531, \"num_examples\": 31, \"num_bytes\": 263840}\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:33 INFO 140442437990208] Finished scoring on 3840 examples from 30 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:33 INFO 140442437990208] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:33 INFO 140442437990208] Loss (name: value) total: 7.86517583529\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:33 INFO 140442437990208] Loss (name: value) kld: 0.0186616375421\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:33 INFO 140442437990208] Loss (name: value) recons: 7.84651417732\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:33 INFO 140442437990208] Loss (name: value) logppx: 7.86517583529\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:33 INFO 140442437990208] #validation_score (2): 7.865175835291544\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:33 INFO 140442437990208] Timing: train: 7.05s, val: 0.53s, epoch: 7.58s\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:33 INFO 140442437990208] #progress_metric: host=algo-2, completed 2 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 124, \"sum\": 124.0, \"min\": 124}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 124, \"sum\": 124.0, \"min\": 124}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 15760, \"sum\": 15760.0, \"min\": 15760}, \"Total Batches Seen\": {\"count\": 1, \"max\": 248, \"sum\": 248.0, \"min\": 248}, \"Total Records Seen\": {\"count\": 1, \"max\": 31520, \"sum\": 31520.0, \"min\": 31520}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 15760, \"sum\": 15760.0, \"min\": 15760}, \"Reset Count\": {\"count\": 1, \"max\": 4, \"sum\": 4.0, \"min\": 4}}, \"EndTime\": 1594734453.540293, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 1}, \"StartTime\": 1594734445.956951}\n",
      "\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:33 INFO 140442437990208] #throughput_metric: host=algo-2, train throughput=2078.20323041 records/second\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:33 INFO 140442437990208] \u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:33 INFO 140442437990208] # Starting training for epoch 3\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[2020-07-14 13:47:32.796] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 5, \"duration\": 6918, \"num_examples\": 124, \"num_bytes\": 1057704}\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:32 INFO 139835197114176] # Finished training epoch 2 on 15756 examples from 124 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:32 INFO 139835197114176] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:32 INFO 139835197114176] Loss (name: value) total: 7.74226925931\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:32 INFO 139835197114176] Loss (name: value) kld: 0.0233560830304\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:32 INFO 139835197114176] Loss (name: value) recons: 7.71891319127\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:32 INFO 139835197114176] Loss (name: value) logppx: 7.74226925931\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:32 INFO 139835197114176] #quality_metric: host=algo-1, epoch=2, train total_loss <loss>=7.74226925931\u001b[0m\n",
      "\u001b[34m[2020-07-14 13:47:33.261] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 5, \"duration\": 464, \"num_examples\": 31, \"num_bytes\": 263840}\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:33 INFO 139835197114176] Finished scoring on 3840 examples from 30 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:33 INFO 139835197114176] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:33 INFO 139835197114176] Loss (name: value) total: 7.86521237691\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:33 INFO 139835197114176] Loss (name: value) kld: 0.0212394496426\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:33 INFO 139835197114176] Loss (name: value) recons: 7.84397296906\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:33 INFO 139835197114176] Loss (name: value) logppx: 7.86521237691\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:33 INFO 139835197114176] #validation_score (2): 7.865212376912435\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:33 INFO 139835197114176] Timing: train: 6.92s, val: 0.47s, epoch: 7.39s\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:33 INFO 139835197114176] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 124, \"sum\": 124.0, \"min\": 124}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 124, \"sum\": 124.0, \"min\": 124}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 15756, \"sum\": 15756.0, \"min\": 15756}, \"Total Batches Seen\": {\"count\": 1, \"max\": 248, \"sum\": 248.0, \"min\": 248}, \"Total Records Seen\": {\"count\": 1, \"max\": 31512, \"sum\": 31512.0, \"min\": 31512}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 15756, \"sum\": 15756.0, \"min\": 15756}, \"Reset Count\": {\"count\": 1, \"max\": 4, \"sum\": 4.0, \"min\": 4}}, \"EndTime\": 1594734453.266703, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 1}, \"StartTime\": 1594734445.877036}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:33 INFO 139835197114176] #throughput_metric: host=algo-1, train throughput=2132.12460827 records/second\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:33 INFO 139835197114176] \u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:33 INFO 139835197114176] # Starting training for epoch 3\u001b[0m\n",
      "\u001b[34m[2020-07-14 13:47:39.870] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 6603, \"num_examples\": 124, \"num_bytes\": 1057704}\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:39 INFO 139835197114176] # Finished training epoch 3 on 15756 examples from 124 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:39 INFO 139835197114176] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:39 INFO 139835197114176] Loss (name: value) total: 7.74678403812\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:39 INFO 139835197114176] Loss (name: value) kld: 0.026313947617\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:39 INFO 139835197114176] Loss (name: value) recons: 7.72047010833\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:39 INFO 139835197114176] Loss (name: value) logppx: 7.74678403812\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:39 INFO 139835197114176] #quality_metric: host=algo-1, epoch=3, train total_loss <loss>=7.74678403812\u001b[0m\n",
      "\u001b[35m[2020-07-14 13:47:40.164] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 6623, \"num_examples\": 124, \"num_bytes\": 1052548}\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:40 INFO 140442437990208] # Finished training epoch 3 on 15760 examples from 124 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:40 INFO 140442437990208] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:40 INFO 140442437990208] Loss (name: value) total: 7.75066617612\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:40 INFO 140442437990208] Loss (name: value) kld: 0.0271970681779\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:40 INFO 140442437990208] Loss (name: value) recons: 7.72346911123\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:40 INFO 140442437990208] Loss (name: value) logppx: 7.75066617612\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:40 INFO 140442437990208] #quality_metric: host=algo-2, epoch=3, train total_loss <loss>=7.75066617612\u001b[0m\n",
      "\u001b[34m[2020-07-14 13:47:40.380] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 8, \"duration\": 508, \"num_examples\": 31, \"num_bytes\": 263840}\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:40 INFO 139835197114176] Finished scoring on 3840 examples from 30 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:40 INFO 139835197114176] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:40 INFO 139835197114176] Loss (name: value) total: 7.8687368234\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:40 INFO 139835197114176] Loss (name: value) kld: 0.0281388587008\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:40 INFO 139835197114176] Loss (name: value) recons: 7.84059791565\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:40 INFO 139835197114176] Loss (name: value) logppx: 7.8687368234\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:40 INFO 139835197114176] #validation_score (3): 7.868736823399861\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:40 INFO 139835197114176] Timing: train: 6.60s, val: 0.51s, epoch: 7.11s\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:40 INFO 139835197114176] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 124, \"sum\": 124.0, \"min\": 124}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 124, \"sum\": 124.0, \"min\": 124}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 15756, \"sum\": 15756.0, \"min\": 15756}, \"Total Batches Seen\": {\"count\": 1, \"max\": 372, \"sum\": 372.0, \"min\": 372}, \"Total Records Seen\": {\"count\": 1, \"max\": 47268, \"sum\": 47268.0, \"min\": 47268}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 15756, \"sum\": 15756.0, \"min\": 15756}, \"Reset Count\": {\"count\": 1, \"max\": 6, \"sum\": 6.0, \"min\": 6}}, \"EndTime\": 1594734460.381861, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 2}, \"StartTime\": 1594734453.266992}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:40 INFO 139835197114176] #throughput_metric: host=algo-1, train throughput=2214.47370962 records/second\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:40 INFO 139835197114176] \u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:40 INFO 139835197114176] # Starting training for epoch 4\u001b[0m\n",
      "\u001b[35m[2020-07-14 13:47:40.781] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 8, \"duration\": 615, \"num_examples\": 31, \"num_bytes\": 263840}\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:40 INFO 140442437990208] Finished scoring on 3840 examples from 30 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:40 INFO 140442437990208] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:40 INFO 140442437990208] Loss (name: value) total: 7.86193091075\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:40 INFO 140442437990208] Loss (name: value) kld: 0.0245398571094\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:40 INFO 140442437990208] Loss (name: value) recons: 7.83739112218\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:40 INFO 140442437990208] Loss (name: value) logppx: 7.86193091075\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:40 INFO 140442437990208] #validation_score (3): 7.861930910746256\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:40 INFO 140442437990208] Timing: train: 6.63s, val: 0.62s, epoch: 7.25s\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:40 INFO 140442437990208] #progress_metric: host=algo-2, completed 3 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 124, \"sum\": 124.0, \"min\": 124}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 124, \"sum\": 124.0, \"min\": 124}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 15760, \"sum\": 15760.0, \"min\": 15760}, \"Total Batches Seen\": {\"count\": 1, \"max\": 372, \"sum\": 372.0, \"min\": 372}, \"Total Records Seen\": {\"count\": 1, \"max\": 47280, \"sum\": 47280.0, \"min\": 47280}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 15760, \"sum\": 15760.0, \"min\": 15760}, \"Reset Count\": {\"count\": 1, \"max\": 6, \"sum\": 6.0, \"min\": 6}}, \"EndTime\": 1594734460.786842, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 2}, \"StartTime\": 1594734453.540523}\n",
      "\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:40 INFO 140442437990208] #throughput_metric: host=algo-2, train throughput=2174.85382794 records/second\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:40 INFO 140442437990208] \u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:40 INFO 140442437990208] # Starting training for epoch 4\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[2020-07-14 13:47:47.102] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 11, \"duration\": 6720, \"num_examples\": 124, \"num_bytes\": 1057704}\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:47 INFO 139835197114176] # Finished training epoch 4 on 15756 examples from 124 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:47 INFO 139835197114176] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:47 INFO 139835197114176] Loss (name: value) total: 7.74336658562\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:47 INFO 139835197114176] Loss (name: value) kld: 0.0297790411127\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:47 INFO 139835197114176] Loss (name: value) recons: 7.71358752491\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:47 INFO 139835197114176] Loss (name: value) logppx: 7.74336658562\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:47 INFO 139835197114176] #quality_metric: host=algo-1, epoch=4, train total_loss <loss>=7.74336658562\u001b[0m\n",
      "\u001b[34m[2020-07-14 13:47:47.612] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 11, \"duration\": 508, \"num_examples\": 31, \"num_bytes\": 263840}\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:47 INFO 139835197114176] Finished scoring on 3840 examples from 30 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:47 INFO 139835197114176] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:47 INFO 139835197114176] Loss (name: value) total: 7.86811920802\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:47 INFO 139835197114176] Loss (name: value) kld: 0.024310683397\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:47 INFO 139835197114176] Loss (name: value) recons: 7.84380852381\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:47 INFO 139835197114176] Loss (name: value) logppx: 7.86811920802\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:47 INFO 139835197114176] #validation_score (4): 7.868119208017985\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:47 INFO 139835197114176] Timing: train: 6.72s, val: 0.51s, epoch: 7.23s\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:47 INFO 139835197114176] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 124, \"sum\": 124.0, \"min\": 124}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 124, \"sum\": 124.0, \"min\": 124}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 15756, \"sum\": 15756.0, \"min\": 15756}, \"Total Batches Seen\": {\"count\": 1, \"max\": 496, \"sum\": 496.0, \"min\": 496}, \"Total Records Seen\": {\"count\": 1, \"max\": 63024, \"sum\": 63024.0, \"min\": 63024}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 15756, \"sum\": 15756.0, \"min\": 15756}, \"Reset Count\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}}, \"EndTime\": 1594734467.61382, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 3}, \"StartTime\": 1594734460.382101}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:47 INFO 139835197114176] #throughput_metric: host=algo-1, train throughput=2178.69350379 records/second\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:47 INFO 139835197114176] \u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:47 INFO 139835197114176] # Starting training for epoch 5\u001b[0m\n",
      "\u001b[35m[2020-07-14 13:47:47.585] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 11, \"duration\": 6797, \"num_examples\": 124, \"num_bytes\": 1052548}\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:47 INFO 140442437990208] # Finished training epoch 4 on 15760 examples from 124 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:47 INFO 140442437990208] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:47 INFO 140442437990208] Loss (name: value) total: 7.74124373063\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:47 INFO 140442437990208] Loss (name: value) kld: 0.0302153713926\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:47 INFO 140442437990208] Loss (name: value) recons: 7.71102838459\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:47 INFO 140442437990208] Loss (name: value) logppx: 7.74124373063\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:47 INFO 140442437990208] #quality_metric: host=algo-2, epoch=4, train total_loss <loss>=7.74124373063\u001b[0m\n",
      "\u001b[35m[2020-07-14 13:47:48.156] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 11, \"duration\": 570, \"num_examples\": 31, \"num_bytes\": 263840}\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:48 INFO 140442437990208] Finished scoring on 3840 examples from 30 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:48 INFO 140442437990208] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:48 INFO 140442437990208] Loss (name: value) total: 7.85885499318\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:48 INFO 140442437990208] Loss (name: value) kld: 0.0264578715588\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:48 INFO 140442437990208] Loss (name: value) recons: 7.83239709536\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:48 INFO 140442437990208] Loss (name: value) logppx: 7.85885499318\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:48 INFO 140442437990208] #validation_score (4): 7.858854993184408\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:48 INFO 140442437990208] Timing: train: 6.80s, val: 0.58s, epoch: 7.37s\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:48 INFO 140442437990208] #progress_metric: host=algo-2, completed 4 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 124, \"sum\": 124.0, \"min\": 124}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 124, \"sum\": 124.0, \"min\": 124}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 15760, \"sum\": 15760.0, \"min\": 15760}, \"Total Batches Seen\": {\"count\": 1, \"max\": 496, \"sum\": 496.0, \"min\": 496}, \"Total Records Seen\": {\"count\": 1, \"max\": 63040, \"sum\": 63040.0, \"min\": 63040}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 15760, \"sum\": 15760.0, \"min\": 15760}, \"Reset Count\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}}, \"EndTime\": 1594734468.162263, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 3}, \"StartTime\": 1594734460.787134}\n",
      "\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:48 INFO 140442437990208] #throughput_metric: host=algo-2, train throughput=2136.86805198 records/second\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:48 INFO 140442437990208] \u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:48 INFO 140442437990208] # Starting training for epoch 5\u001b[0m\n",
      "\u001b[34m[2020-07-14 13:47:54.165] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 6551, \"num_examples\": 124, \"num_bytes\": 1057704}\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:54 INFO 139835197114176] # Finished training epoch 5 on 15756 examples from 124 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:54 INFO 139835197114176] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:54 INFO 139835197114176] Loss (name: value) total: 7.73785609199\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:54 INFO 139835197114176] Loss (name: value) kld: 0.0325111557876\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:54 INFO 139835197114176] Loss (name: value) recons: 7.70534495481\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:54 INFO 139835197114176] Loss (name: value) logppx: 7.73785609199\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:54 INFO 139835197114176] #quality_metric: host=algo-1, epoch=5, train total_loss <loss>=7.73785609199\u001b[0m\n",
      "\u001b[34m[2020-07-14 13:47:54.637] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 14, \"duration\": 470, \"num_examples\": 31, \"num_bytes\": 263840}\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:54 INFO 139835197114176] Finished scoring on 3840 examples from 30 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:54 INFO 139835197114176] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:54 INFO 139835197114176] Loss (name: value) total: 7.86092368762\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:54 INFO 139835197114176] Loss (name: value) kld: 0.0295461854587\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:54 INFO 139835197114176] Loss (name: value) recons: 7.83137752215\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:54 INFO 139835197114176] Loss (name: value) logppx: 7.86092368762\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:54 INFO 139835197114176] #validation_score (5): 7.860923687616984\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:54 INFO 139835197114176] Timing: train: 6.55s, val: 0.48s, epoch: 7.03s\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:54 INFO 139835197114176] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 124, \"sum\": 124.0, \"min\": 124}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 124, \"sum\": 124.0, \"min\": 124}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 15756, \"sum\": 15756.0, \"min\": 15756}, \"Total Batches Seen\": {\"count\": 1, \"max\": 620, \"sum\": 620.0, \"min\": 620}, \"Total Records Seen\": {\"count\": 1, \"max\": 78780, \"sum\": 78780.0, \"min\": 78780}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 15756, \"sum\": 15756.0, \"min\": 15756}, \"Reset Count\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}}, \"EndTime\": 1594734474.642017, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 4}, \"StartTime\": 1594734467.6141}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:54 INFO 139835197114176] #throughput_metric: host=algo-1, train throughput=2241.87593381 records/second\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:54 INFO 139835197114176] \u001b[0m\n",
      "\u001b[34m[07/14/2020 13:47:54 INFO 139835197114176] # Starting training for epoch 6\u001b[0m\n",
      "\u001b[35m[2020-07-14 13:47:54.740] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 6577, \"num_examples\": 124, \"num_bytes\": 1052548}\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:54 INFO 140442437990208] # Finished training epoch 5 on 15760 examples from 124 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:54 INFO 140442437990208] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:54 INFO 140442437990208] Loss (name: value) total: 7.73947843525\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:54 INFO 140442437990208] Loss (name: value) kld: 0.032527314867\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:54 INFO 140442437990208] Loss (name: value) recons: 7.70695111925\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:54 INFO 140442437990208] Loss (name: value) logppx: 7.73947843525\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:54 INFO 140442437990208] #quality_metric: host=algo-2, epoch=5, train total_loss <loss>=7.73947843525\u001b[0m\n",
      "\u001b[35m[2020-07-14 13:47:55.282] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 14, \"duration\": 539, \"num_examples\": 31, \"num_bytes\": 263840}\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:55 INFO 140442437990208] Finished scoring on 3840 examples from 30 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:55 INFO 140442437990208] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:55 INFO 140442437990208] Loss (name: value) total: 7.84721420606\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:55 INFO 140442437990208] Loss (name: value) kld: 0.0271735700468\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:55 INFO 140442437990208] Loss (name: value) recons: 7.82004067103\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:55 INFO 140442437990208] Loss (name: value) logppx: 7.84721420606\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:55 INFO 140442437990208] #validation_score (5): 7.847214206059774\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:55 INFO 140442437990208] Timing: train: 6.58s, val: 0.54s, epoch: 7.12s\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:55 INFO 140442437990208] #progress_metric: host=algo-2, completed 5 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 124, \"sum\": 124.0, \"min\": 124}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 124, \"sum\": 124.0, \"min\": 124}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 15760, \"sum\": 15760.0, \"min\": 15760}, \"Total Batches Seen\": {\"count\": 1, \"max\": 620, \"sum\": 620.0, \"min\": 620}, \"Total Records Seen\": {\"count\": 1, \"max\": 78800, \"sum\": 78800.0, \"min\": 78800}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 15760, \"sum\": 15760.0, \"min\": 15760}, \"Reset Count\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}}, \"EndTime\": 1594734475.287094, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 4}, \"StartTime\": 1594734468.162543}\n",
      "\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:55 INFO 140442437990208] #throughput_metric: host=algo-2, train throughput=2212.01953579 records/second\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:55 INFO 140442437990208] \u001b[0m\n",
      "\u001b[35m[07/14/2020 13:47:55 INFO 140442437990208] # Starting training for epoch 6\u001b[0m\n",
      "\u001b[34m[2020-07-14 13:48:01.248] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 17, \"duration\": 6606, \"num_examples\": 124, \"num_bytes\": 1057704}\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:48:01 INFO 139835197114176] # Finished training epoch 6 on 15756 examples from 124 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:48:01 INFO 139835197114176] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:48:01 INFO 139835197114176] Loss (name: value) total: 7.73933013264\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:48:01 INFO 139835197114176] Loss (name: value) kld: 0.0349356443743\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:48:01 INFO 139835197114176] Loss (name: value) recons: 7.70439450683\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:48:01 INFO 139835197114176] Loss (name: value) logppx: 7.73933013264\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:48:01 INFO 139835197114176] #quality_metric: host=algo-1, epoch=6, train total_loss <loss>=7.73933013264\u001b[0m\n",
      "\u001b[34m[2020-07-14 13:48:01.762] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 17, \"duration\": 511, \"num_examples\": 31, \"num_bytes\": 263840}\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:48:01 INFO 139835197114176] Finished scoring on 3840 examples from 30 batches, each of size 128.\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:48:01 INFO 139835197114176] Metrics for Inference:\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:48:01 INFO 139835197114176] Loss (name: value) total: 7.86707100868\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:48:01 INFO 139835197114176] Loss (name: value) kld: 0.031241775987\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:48:01 INFO 139835197114176] Loss (name: value) recons: 7.83582922618\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:48:01 INFO 139835197114176] Loss (name: value) logppx: 7.86707100868\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:48:01 INFO 139835197114176] #validation_score (6): 7.867071008682251\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:48:01 INFO 139835197114176] patience losses:[7.865766445795695, 7.865212376912435, 7.868736823399861, 7.868119208017985, 7.860923687616984] min patience loss:7.86092368762 current loss:7.86707100868 absolute loss difference:0.00614732106527\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:48:01 INFO 139835197114176] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:48:01 INFO 139835197114176] Timing: train: 6.61s, val: 0.51s, epoch: 7.12s\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:48:01 INFO 139835197114176] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 124, \"sum\": 124.0, \"min\": 124}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 124, \"sum\": 124.0, \"min\": 124}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 15756, \"sum\": 15756.0, \"min\": 15756}, \"Total Batches Seen\": {\"count\": 1, \"max\": 744, \"sum\": 744.0, \"min\": 744}, \"Total Records Seen\": {\"count\": 1, \"max\": 94536, \"sum\": 94536.0, \"min\": 94536}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 15756, \"sum\": 15756.0, \"min\": 15756}, \"Reset Count\": {\"count\": 1, \"max\": 12, \"sum\": 12.0, \"min\": 12}}, \"EndTime\": 1594734481.76377, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 5}, \"StartTime\": 1594734474.642253}\n",
      "\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:48:01 INFO 139835197114176] #throughput_metric: host=algo-1, train throughput=2212.40486989 records/second\u001b[0m\n",
      "\u001b[34m[07/14/2020 13:48:01 INFO 139835197114176] \u001b[0m\n",
      "\u001b[34m[07/14/2020 13:48:01 INFO 139835197114176] # Starting training for epoch 7\u001b[0m\n",
      "\u001b[35m[2020-07-14 13:48:02.056] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 17, \"duration\": 6766, \"num_examples\": 124, \"num_bytes\": 1052548}\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:48:02 INFO 140442437990208] # Finished training epoch 6 on 15760 examples from 124 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:48:02 INFO 140442437990208] Metrics for Training:\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:48:02 INFO 140442437990208] Loss (name: value) total: 7.73500298973\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:48:02 INFO 140442437990208] Loss (name: value) kld: 0.0350545830874\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:48:02 INFO 140442437990208] Loss (name: value) recons: 7.69994840314\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:48:02 INFO 140442437990208] Loss (name: value) logppx: 7.73500298973\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:48:02 INFO 140442437990208] #quality_metric: host=algo-2, epoch=6, train total_loss <loss>=7.73500298973\u001b[0m\n",
      "\u001b[35m[2020-07-14 13:48:02.596] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/validation\", \"epoch\": 17, \"duration\": 538, \"num_examples\": 31, \"num_bytes\": 263840}\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:48:02 INFO 140442437990208] Finished scoring on 3840 examples from 30 batches, each of size 128.\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:48:02 INFO 140442437990208] Metrics for Inference:\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:48:02 INFO 140442437990208] Loss (name: value) total: 7.84660183589\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:48:02 INFO 140442437990208] Loss (name: value) kld: 0.0296373046314\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:48:02 INFO 140442437990208] Loss (name: value) recons: 7.81696451505\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:48:02 INFO 140442437990208] Loss (name: value) logppx: 7.84660183589\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:48:02 INFO 140442437990208] #validation_score (6): 7.846601835886637\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:48:02 INFO 140442437990208] patience losses:[7.863290675481161, 7.865175835291544, 7.861930910746256, 7.858854993184408, 7.847214206059774] min patience loss:7.84721420606 current loss:7.84660183589 absolute loss difference:0.000612370173137\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:48:02 INFO 140442437990208] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:48:02 INFO 140442437990208] Timing: train: 6.77s, val: 0.54s, epoch: 7.31s\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:48:02 INFO 140442437990208] #progress_metric: host=algo-2, completed 6 % of epochs\u001b[0m\n",
      "\u001b[35m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 124, \"sum\": 124.0, \"min\": 124}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 124, \"sum\": 124.0, \"min\": 124}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 15760, \"sum\": 15760.0, \"min\": 15760}, \"Total Batches Seen\": {\"count\": 1, \"max\": 744, \"sum\": 744.0, \"min\": 744}, \"Total Records Seen\": {\"count\": 1, \"max\": 94560, \"sum\": 94560.0, \"min\": 94560}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 15760, \"sum\": 15760.0, \"min\": 15760}, \"Reset Count\": {\"count\": 1, \"max\": 12, \"sum\": 12.0, \"min\": 12}}, \"EndTime\": 1594734482.602001, \"Dimensions\": {\"Host\": \"algo-2\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 5}, \"StartTime\": 1594734475.287506}\n",
      "\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:48:02 INFO 140442437990208] #throughput_metric: host=algo-2, train throughput=2154.55369894 records/second\u001b[0m\n",
      "\u001b[35m[07/14/2020 13:48:02 INFO 140442437990208] \u001b[0m\n",
      "\u001b[35m[07/14/2020 13:48:02 INFO 140442437990208] # Starting training for epoch 7\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ntm.fit({'train': s3_train, 'validation': s3_val_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see the message\n",
    "\n",
    "> `===== Job Complete =====`\n",
    "\n",
    "at the bottom of the output logs then that means training successfully completed and the output NTM model was stored in the specified output path. You can also view information about and the status of a training job using the AWS SageMaker console. Just click on the \"Jobs\" tab and select training job matching the training job name, below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training job name: {}'.format(ntm.latest_training_job.job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Hosting and Inference\n",
    "\n",
    "A trained NTM model does nothing on its own. We now want to use the model we computed to perform inference on data. For this example, that means predicting the topic mixture representing a given document.\n",
    "\n",
    "We create an inference endpoint using the SageMaker Python SDK `deploy()` function from the job we defined above. We specify the instance type where inference is computed as well as an initial number of instances to spin up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntm_predictor = ntm.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You now have a functioning SageMaker NTM inference endpoint. You can confirm the endpoint configuration and status by navigating to the \"Endpoints\" tab in the AWS SageMaker console and selecting the endpoint matching the endpoint name, below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Endpoint name: {}'.format(ntm_predictor.endpoint))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Serialization/Deserialization\n",
    "\n",
    "We can pass data in a variety of formats to our inference endpoint. First, we will demonstrate passing CSV-formatted data. We make use of the SageMaker Python SDK utilities `csv_serializer` and `json_deserializer` when configuring the inference endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference with CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "\n",
    "ntm_predictor.content_type = 'text/csv'\n",
    "ntm_predictor.serializer = csv_serializer\n",
    "ntm_predictor.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pass 5 examples from the test set to the inference endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.array(test_vectors.todense())\n",
    "results = ntm_predictor.predict(test_data[:12])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the output format of SageMaker NTM inference endpoint is a Python dictionary with the following format.\n",
    "\n",
    "```\n",
    "{\n",
    "  'predictions': [\n",
    "    {'topic_weights': [ ... ] },\n",
    "    {'topic_weights': [ ... ] },\n",
    "    {'topic_weights': [ ... ] },\n",
    "    ...\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "We extract the topic weights, themselves, corresponding to each of the input documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array([prediction['topic_weights'] for prediction in results['predictions']])\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Now we can take a look at how the 20 topics are assigned to the 5 test documents with a bar plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fs = 12\n",
    "df=pd.DataFrame(predictions.T)\n",
    "df.plot(kind='bar', figsize=(16,4), fontsize=fs)\n",
    "plt.ylabel('Topic assignment', fontsize=fs+2)\n",
    "plt.xlabel('Topic ID', fontsize=fs+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop / Close the Endpoint\n",
    "\n",
    "Finally, we should delete the endpoint before we close the notebook.\n",
    "\n",
    "To restart the endpoint you can follow the code above using the same `endpoint_name` we created or you can navigate to the \"Endpoints\" tab in the SageMaker console, select the endpoint with the name stored in the variable `endpoint_name`, and select \"Delete\" from the \"Actions\" dropdown menu. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(ntm_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note: The following section is meant as a deeper dive into exploring the trained models. The demonstrated functionalities may not be fully supported or guaranteed. For example, the parameter names may change without notice.***\n",
    "\n",
    "\n",
    "The trained model artifact is a compressed package of MXNet models from the two workers. To explore the model, we first need to install mxnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you use conda_mxnet_p36 kernel, mxnet is already installed, otherwise, uncomment the following line to install.\n",
    "# !pip install mxnet \n",
    "import mxnet as mx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we download unpack the artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(output_prefix, ntm._current_job_name, 'output/model.tar.gz')\n",
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.resource('s3').Bucket(bucket).download_file(model_path, 'downloaded_model.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xzvf 'downloaded_model.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use flag -o to overwrite previous unzipped content\n",
    "!unzip -o model_algo-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load the model parameters and extract the weight matrix $W$ in the decoder as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mx.ndarray.load('params')\n",
    "W = model['arg:projection_weight']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix $W$ corresponds to the $W$ in the NTM digram at the beginning of this notebook. Each column of $W$ corresponds to a learned topic. The elements in the columns of $W$ corresponds to the pseudo-probability of a word within a topic. We can visualize each topic as a word cloud with the size of each word be proportional to the pseudo-probability of the words appearing under each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud\n",
    "import wordcloud as wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = dict()\n",
    "for i, v in enumerate(vocab_list):\n",
    "    word_to_id[v] = i\n",
    "\n",
    "limit = 24\n",
    "n_col = 4\n",
    "counter = 0\n",
    "font_path = '/usr/share/fonts/cjkuni-ukai/ukai.ttc'\n",
    "plt.figure(figsize=(20,16))\n",
    "for ind in range(num_topics):\n",
    "\n",
    "    if counter >= limit:\n",
    "        break\n",
    "\n",
    "    title_str = 'Topic{}'.format(ind)\n",
    "\n",
    "    #pvals = mx.nd.softmax(W[:, ind]).asnumpy()\n",
    "    pvals = mx.nd.softmax(mx.nd.array(W[:, ind])).asnumpy()\n",
    "\n",
    "    word_freq = dict()\n",
    "    for k in word_to_id.keys():\n",
    "        i = word_to_id[k]\n",
    "        word_freq[k] =pvals[i]\n",
    "\n",
    "    wordcloud = wc.WordCloud(font_path=font_path, background_color='white').fit_words(word_freq)\n",
    "\n",
    "    plt.subplot(limit // n_col, n_col, counter+1)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title_str)\n",
    "    #plt.close()\n",
    "\n",
    "    counter +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
